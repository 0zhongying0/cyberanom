{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Language model to process user events\n",
    "Bidirectional LSTM model with dropout and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.7.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "data_dir = 'data/char_feats/U1480/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform character-based input into equivalent numerical versions\n",
    "def encode_data(text, num_chars, max_length):\n",
    "    # create empty vessels for one-hot encoded input\n",
    "    X = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    y = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # loop over inputs and tranform and store in X\n",
    "    for i, sentence in enumerate(text):\n",
    "        sentence = '\\t' + sentence + '\\n'\n",
    "        for j, c in enumerate(sentence):\n",
    "            X[i, j, ord(c)] = 1\n",
    "            if j > 0:\n",
    "                # target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                y[i, j - 1, ord(c)] = 1.\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname, max_len):\n",
    "    \"\"\"\n",
    "        process file by extracting sentences data and encode them producing \n",
    "        a set of input and target data for processing by the model\n",
    "        'fname' contains coma separated fields where the last one is the \n",
    "        sentence to be processes\n",
    "    \"\"\"\n",
    "    data = open(fname).read()\n",
    "\n",
    "    text = []\n",
    "    red_events = []\n",
    "    max_text_len = 0\n",
    "    with open(dataset_fname, 'r') as infile:\n",
    "        for i, line in enumerate(infile.readlines()):\n",
    "            line = line.strip().split(',')\n",
    "            text.append(line[-1])\n",
    "            max_text_len = max(max_text_len, int(line[-2]))\n",
    "            if int(line[2]) == 1:\n",
    "                red_events.append((i,line))\n",
    "\n",
    "#     print(text[0], 'len:', len(text[0]), len(text))\n",
    "    print('max_input_lenght:', max_text_len)\n",
    "    input_data, target_data = encode_data(text, 128, max_len)\n",
    "    \n",
    "    return input_data, target_data, red_events\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model using Keras\n",
    "\n",
    "The TensorFlow [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together. See the [Keras documentation](https://keras.io/) for details.\n",
    "\n",
    "The [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, one [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) and one [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers with 'num_chars' nodes each. The first layer's `input_shape` parameter corresponds to the amount of features from the dataset, and is required.\n",
    "\n",
    "#### Define the loss and gradient function\n",
    "\n",
    "Both training and evaluation stages need to calculate the model's *[loss](https://developers.google.com/machine-learning/crash-course/glossary#loss)*. This measures how off a model's predictions are from the desired output. We want to minimize, or optimize, this value.\n",
    "\n",
    "Our model will calculate its loss using the [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) function which takes the model's prediction and the desired output. The returned loss value is progressively larger as the prediction gets worse.\n",
    "\n",
    "The `grad` function uses the `loss` function and the [tfe.GradientTape](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/GradientTape) to record operations that compute the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = 128 # our vocabulary, i.e. unique characters in text. We'll just use the first 128 (half ASCII)\n",
    "\n",
    "def getModel(max_len):\n",
    "    model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.LSTM(120, input_shape=(None, num_chars), return_sequences=True),  # input shape required\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(max_len, return_sequences=True), input_shape=(None, num_chars)),  # input shape required\n",
    "#         tf.keras.layers.Dense(240, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_chars, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.keras.losses.categorical_crossentropy(y, y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer\n",
    "TensorFlow has many [optimization algorithms](https://www.tensorflow.org/api_guides/python/train) available for training. This model uses the [tf.train.GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) that implements the *[stochastic gradient descent](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)* (SGD) algorithm. The `learning_rate` sets the step size to take for each iteration down the hill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, epsilon=1e-08)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_lenght: 68\n",
      "processing: data/char_feats/U1480/0.txt  - num events: 629  - red events: 0\n",
      "Epoch 000: Loss: 2.214, Accuracy: 0 - in: 27.797 sec.\n",
      "Epoch 001: Loss: 1.654, Accuracy: 0 - in: 27.555 sec.\n",
      "  avg_loss: tf.Tensor(1.9197483, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.2815318, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/1.txt  - num events: 497  - red events: 0\n",
      "  possible anomalies using avg: 0 - using max: 0 - eval_max_loss: 1.8602780103683472\n",
      "    max: []\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/1.txt  - num events: 497  - red events: 0\n",
      "Epoch 000: Loss: 1.501, Accuracy: 0 - in: 20.201 sec.\n",
      "Epoch 001: Loss: 1.414, Accuracy: 0 - in: 20.162 sec.\n",
      "  avg_loss: tf.Tensor(1.4566956, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.5874922, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/2.txt  - num events: 793  - red events: 0\n",
      "  possible anomalies using avg: 155 - using max: 12 - eval_max_loss: 1.6648136377334595\n",
      "    max: [(297, 1.6648136377334595), (334, 1.6648136377334595), (708, 1.6648136377334595), (246, 1.6170886754989624), (494, 1.596697449684143), (540, 1.596697449684143), (542, 1.596697449684143), (545, 1.596697449684143), (546, 1.596697449684143), (548, 1.596697449684143)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/2.txt  - num events: 793  - red events: 0\n",
      "Epoch 000: Loss: 1.285, Accuracy: 0 - in: 33.162 sec.\n",
      "Epoch 001: Loss: 1.235, Accuracy: 0 - in: 32.482 sec.\n",
      "  avg_loss: tf.Tensor(1.259179, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.3557907, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/3.txt  - num events: 635  - red events: 0\n",
      "  possible anomalies using avg: 220 - using max: 123 - eval_max_loss: 1.4941881895065308\n",
      "    max: [(549, 1.4941881895065308), (565, 1.4941881895065308), (567, 1.4941881895065308), (575, 1.4941881895065308), (581, 1.4941881895065308), (593, 1.4941881895065308), (594, 1.4941881895065308), (621, 1.4941881895065308), (623, 1.4941881895065308), (9, 1.4843918085098267)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/3.txt  - num events: 635  - red events: 0\n",
      "Epoch 000: Loss: 1.183, Accuracy: 0 - in: 28.292 sec.\n",
      "Epoch 001: Loss: 1.160, Accuracy: 0 - in: 28.170 sec.\n",
      "  avg_loss: tf.Tensor(1.1638819, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.2830968, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/4.txt  - num events: 647  - red events: 0\n",
      "  possible anomalies using avg: 389 - using max: 143 - eval_max_loss: 1.477509617805481\n",
      "    max: [(31, 1.477509617805481), (3, 1.4458938837051392), (16, 1.4458938837051392), (33, 1.4458938837051392), (36, 1.4458938837051392), (45, 1.4458938837051392), (49, 1.4458938837051392), (67, 1.4458938837051392), (80, 1.4458938837051392), (89, 1.4458938837051392)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/4.txt  - num events: 647  - red events: 0\n",
      "Epoch 000: Loss: 1.111, Accuracy: 0 - in: 28.124 sec.\n",
      "Epoch 001: Loss: 1.092, Accuracy: 0 - in: 28.793 sec.\n",
      "  avg_loss: tf.Tensor(1.0963622, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.1600679, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/5.txt  - num events: 680  - red events: 0\n",
      "  possible anomalies using avg: 157 - using max: 149 - eval_max_loss: 1.3782345056533813\n",
      "    max: [(357, 1.3782345056533813), (437, 1.2918140888214111), (444, 1.2905138731002808), (446, 1.2905138731002808), (447, 1.2905138731002808), (448, 1.2905138731002808), (1, 1.2612937688827515), (14, 1.2612937688827515), (16, 1.2612937688827515), (17, 1.2612937688827515)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/5.txt  - num events: 680  - red events: 0\n",
      "Epoch 000: Loss: 1.031, Accuracy: 0 - in: 29.424 sec.\n",
      "Epoch 001: Loss: 0.974, Accuracy: 0 - in: 28.235 sec.\n",
      "  avg_loss: tf.Tensor(1.0014654, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.0785786, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/6.txt  - num events: 810  - red events: 0\n",
      "  possible anomalies using avg: 180 - using max: 171 - eval_max_loss: 1.255937933921814\n",
      "    max: [(552, 1.255937933921814), (672, 1.228027105331421), (673, 1.228027105331421), (260, 1.2017050981521606), (314, 1.2017050981521606), (317, 1.2017050981521606), (343, 1.2017050981521606), (515, 1.2017050981521606), (626, 1.2017050981521606), (461, 1.1775119304656982)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/6.txt  - num events: 810  - red events: 0\n",
      "Epoch 000: Loss: 0.890, Accuracy: 0 - in: 37.397 sec.\n",
      "Epoch 001: Loss: 0.832, Accuracy: 0 - in: 37.715 sec.\n",
      "  avg_loss: tf.Tensor(0.86054933, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.96415883, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/7.txt  - num events: 1067  - red events: 0\n",
      "  possible anomalies using avg: 414 - using max: 249 - eval_max_loss: 1.1818084716796875\n",
      "    max: [(275, 1.1818084716796875), (527, 1.1818084716796875), (720, 1.1818084716796875), (862, 1.1818084716796875), (685, 1.1536216735839844), (294, 1.109268069267273), (297, 1.109268069267273), (298, 1.109268069267273), (385, 1.096575140953064), (956, 1.096575140953064)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/7.txt  - num events: 1067  - red events: 0\n",
      "Epoch 000: Loss: 0.778, Accuracy: 0 - in: 46.382 sec.\n",
      "Epoch 001: Loss: 0.725, Accuracy: 0 - in: 46.481 sec.\n",
      "  avg_loss: tf.Tensor(0.7499023, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.85586196, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/8.txt  - num events: 1125  - red events: 10\n",
      "... score for red event: 0.938208818435669\n",
      "... score for red event: 0.9607763886451721\n",
      "... score for red event: 0.9268293976783752\n",
      "... score for red event: 0.9508668780326843\n",
      "... score for red event: 0.9359719753265381\n",
      "... score for red event: 0.9477378726005554\n",
      "... score for red event: 0.9301524758338928\n",
      "... score for red event: 0.8536881804466248\n",
      "... score for red event: 0.9477378726005554\n",
      "... score for red event: 0.9254165887832642\n",
      "  possible anomalies using avg: 281 - using max: 48 - eval_max_loss: 1.2432957887649536\n",
      "    max: [(994, 1.2432957887649536), (995, 1.231030821800232), (981, 1.1963621377944946), (303, 1.036368727684021), (599, 0.9989343285560608), (683, 0.9989343285560608), (276, 0.9949359893798828), (304, 0.9753357172012329), (438, 0.9753357172012329), (574, 0.9753357172012329)]\n",
      "    red events: [960, 962, 971, 972, 983, 984, 986, 991, 992, 999]\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/8.txt  - num events: 1125  - red events: 10\n",
      "Epoch 000: Loss: 0.651, Accuracy: 0 - in: 49.649 sec.\n",
      "Epoch 001: Loss: 0.616, Accuracy: 0 - in: 49.748 sec.\n",
      "  avg_loss: tf.Tensor(0.6322928, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.73761076, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/9.txt  - num events: 709  - red events: 0\n",
      "  possible anomalies using avg: 177 - using max: 1 - eval_max_loss: 0.7454962134361267\n",
      "    max: [(538, 0.7454962134361267)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/9.txt  - num events: 709  - red events: 0\n",
      "Epoch 000: Loss: 0.557, Accuracy: 0 - in: 32.717 sec.\n",
      "Epoch 001: Loss: 0.542, Accuracy: 0 - in: 32.993 sec.\n",
      "  avg_loss: tf.Tensor(0.54346275, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.6365603, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/10.txt  - num events: 670  - red events: 0\n",
      "  possible anomalies using avg: 179 - using max: 9 - eval_max_loss: 0.6397684812545776\n",
      "    max: [(174, 0.6397684812545776), (237, 0.6397684812545776), (293, 0.6397684812545776), (294, 0.6397684812545776), (343, 0.6397684812545776), (387, 0.6397684812545776), (427, 0.6397684812545776), (472, 0.6397684812545776), (474, 0.6397684812545776)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/10.txt  - num events: 670  - red events: 0\n",
      "Epoch 000: Loss: 0.507, Accuracy: 0 - in: 28.619 sec.\n",
      "Epoch 001: Loss: 0.467, Accuracy: 0 - in: 29.158 sec.\n",
      "  avg_loss: tf.Tensor(0.48620006, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.531077, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/11.txt  - num events: 821  - red events: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  possible anomalies using avg: 237 - using max: 210 - eval_max_loss: 0.5861659646034241\n",
      "    max: [(57, 0.5861659646034241), (19, 0.5835021138191223), (46, 0.5719260573387146), (291, 0.5570013523101807), (320, 0.5570013523101807), (333, 0.5570013523101807), (341, 0.5570013523101807), (353, 0.5570013523101807), (382, 0.5570013523101807), (396, 0.5570013523101807)]\n",
      "    red events: []\n",
      "max_input_lenght: 65\n",
      "processing: data/char_feats/U1480/11.txt  - num events: 821  - red events: 0\n",
      "Epoch 000: Loss: 0.451, Accuracy: 0 - in: 37.425 sec.\n",
      "Epoch 001: Loss: 0.421, Accuracy: 0 - in: 37.539 sec.\n",
      "  avg_loss: tf.Tensor(0.43445098, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.52010286, shape=(), dtype=float32)\n",
      "max_input_lenght: 65\n",
      "  evaluating: data/char_feats/U1480/12.txt  - num events: 808  - red events: 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "\n",
    "# keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "max_len = 120 # length of sentence\n",
    "\n",
    "model = getModel(max_len)\n",
    "\n",
    "for i in range(13):\n",
    "    dataset_fname = data_dir+'{0}.txt'.format(i)\n",
    "    input_data, target_data, red_events = process_file(dataset_fname, max_len)\n",
    "#     print(input_data.shape)\n",
    "#     print(target_data.shape)\n",
    "    print('processing:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "    training_dataset = training_dataset.batch(batch_size)\n",
    "#     print(training_dataset)\n",
    "    train_losses = [];\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg = tfe.metrics.Mean()\n",
    "        epoch_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "        startTime = time.time()\n",
    "        # training using batches of 'batch_size'\n",
    "        for X, y in tfe.Iterator(training_dataset):\n",
    "            grads = grad(model, X, y)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables), \n",
    "                                     global_step=tf.train.get_or_create_global_step())\n",
    "            batch_loss = loss(model, X, y)\n",
    "            epoch_loss_avg(batch_loss) # batch loss\n",
    "            train_losses.append(tf.reduce_mean(batch_loss))\n",
    "#             epoch_accuracy(model(X), y)\n",
    "\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "#         train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: 0 - in: {:.3f} sec.\".format(epoch, \n",
    "                                                                        epoch_loss_avg.result(), \n",
    "#                                                                         epoch_accuracy.result(),\n",
    "                                                                        (time.time()-startTime)))        \n",
    "\n",
    "    avg_loss = tf.reduce_mean(train_losses)\n",
    "    max_loss = tf.reduce_max(train_losses)\n",
    "    print('  avg_loss:', avg_loss, ' - max_loss:', max_loss)\n",
    "    dataset_fname = data_dir+'{0}.txt'.format(i+1)\n",
    "    input_data, target_data, red_events = process_file(dataset_fname, max_len)\n",
    "#     print(input_data.shape)\n",
    "#     print(target_data.shape)\n",
    "    print('  evaluating:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "    eval_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "    eval_dataset = eval_dataset.batch(batch_size)\n",
    "\n",
    "    line_losses = np.array([])\n",
    "\n",
    "    # eval using batches of 'batch_size'\n",
    "    for X, y in tfe.Iterator(eval_dataset):\n",
    "        line_losses = np.append(line_losses, tf.reduce_mean(loss(model, X, y), axis=1))\n",
    "    \n",
    "\n",
    "    eval_max_loss = 0;\n",
    "    possible_anomalies_avg = []\n",
    "    possible_anomalies_max = []\n",
    "    for i, v in enumerate(line_losses):\n",
    "        if v > avg_loss:\n",
    "            possible_anomalies_avg.append((i, v))\n",
    "        if v > max_loss:\n",
    "            possible_anomalies_max.append((i, v))\n",
    "        eval_max_loss = max(eval_max_loss, v)\n",
    "    \n",
    "    for a,b in red_events:\n",
    "        for i, v in enumerate(line_losses):\n",
    "            if a == i:\n",
    "                print('... score for red event:', v)\n",
    "                \n",
    "    possible_anomalies_avg.sort(key=lambda x: x[1], reverse=True)\n",
    "    possible_anomalies_max.sort(key=lambda x: x[1], reverse=True)\n",
    "    print('  possible anomalies using avg:', len(possible_anomalies_avg), \n",
    "          '- using max:', len(possible_anomalies_max),\n",
    "          '- eval_max_loss:', eval_max_loss)\n",
    "#     print('    avg:', possible_anomalies_avg[:10])\n",
    "    print('    max:', possible_anomalies_max[:10])\n",
    "    print('    red events:', [a for a,b in red_events])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "# axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "# axes[1].plot(train_accuracy_results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = 'models/model_lm_v1.hdfs'\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    model_filepath,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Language model to process user events\n",
    "Bidirectional LSTM model with dropout and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.7.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "data_dir = 'data/char_feats/U12/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform character-based input into equivalent numerical versions\n",
    "def encode_data(text, num_chars, max_length):\n",
    "    # create empty vessels for one-hot encoded input\n",
    "    X = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    y = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # loop over inputs and tranform and store in X\n",
    "    for i, sentence in enumerate(text):\n",
    "        sentence = '\\t' + sentence + '\\n'\n",
    "        for j, c in enumerate(sentence):\n",
    "            X[i, j, ord(c)] = 1\n",
    "            if j > 0:\n",
    "                # target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                y[i, j - 1, ord(c)] = 1.\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname, max_len):\n",
    "    \"\"\"\n",
    "        process file by extracting sentences data and encode them producing \n",
    "        a set of input and target data for processing by the model\n",
    "        'fname' contains coma separated fields where the last one is the \n",
    "        sentence to be processes\n",
    "    \"\"\"\n",
    "    data = open(fname).read()\n",
    "\n",
    "    text = []\n",
    "    red_events = []\n",
    "    max_text_len = 0\n",
    "    with open(dataset_fname, 'r') as infile:\n",
    "        for i, line in enumerate(infile.readlines()):\n",
    "            line = line.strip().split(',')\n",
    "            text.append(line[-1])\n",
    "            max_text_len = max(max_text_len, int(line[-2]))\n",
    "            if int(line[2]) == 1:\n",
    "                red_events.append((i,line))\n",
    "\n",
    "#     print(text[0], 'len:', len(text[0]), len(text))\n",
    "    print('max_input_lenght:', max_text_len)\n",
    "    input_data, target_data = encode_data(text, 128, max_len)\n",
    "    \n",
    "    return input_data, target_data, red_events\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model using Keras\n",
    "\n",
    "The TensorFlow [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together. See the [Keras documentation](https://keras.io/) for details.\n",
    "\n",
    "The [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, one [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) and one [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers with 'num_chars' nodes each. The first layer's `input_shape` parameter corresponds to the amount of features from the dataset, and is required.\n",
    "\n",
    "#### Define the loss and gradient function\n",
    "\n",
    "Both training and evaluation stages need to calculate the model's *[loss](https://developers.google.com/machine-learning/crash-course/glossary#loss)*. This measures how off a model's predictions are from the desired output. We want to minimize, or optimize, this value.\n",
    "\n",
    "Our model will calculate its loss using the [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) function which takes the model's prediction and the desired output. The returned loss value is progressively larger as the prediction gets worse.\n",
    "\n",
    "The `grad` function uses the `loss` function and the [tfe.GradientTape](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/GradientTape) to record operations that compute the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = 128 # our vocabulary, i.e. unique characters in text. We'll just use the first 128 (half ASCII)\n",
    "\n",
    "def getModel(max_len):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0., input_shape=(None, num_chars)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(max_len, return_sequences=True)),  # input shape required\n",
    "#         tf.keras.layers.Dense(240, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_chars, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.keras.losses.categorical_crossentropy(y, y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer\n",
    "TensorFlow has many [optimization algorithms](https://www.tensorflow.org/api_guides/python/train) available for training. This model uses the [tf.train.GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) that implements the *[stochastic gradient descent](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)* (SGD) algorithm. The `learning_rate` sets the step size to take for each iteration down the hill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, epsilon=1e-08)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/0.txt  - num events: 3128  - red events: 0\n",
      "Epoch 000: Loss: 1.647, Accuracy: 0 - in: 99.731 sec.\n",
      "Epoch 001: Loss: 1.227, Accuracy: 0 - in: 99.179 sec.\n",
      "  avg_loss: tf.Tensor(1.431376, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.184606, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/1.txt  - num events: 8355  - red events: 0\n",
      "  possible anomalies using avg: 0 - using max: 0 - eval_max_loss: 1.3660649061203003\n",
      "    max: []\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/1.txt  - num events: 8355  - red events: 0\n",
      "Epoch 000: Loss: 1.024, Accuracy: 0 - in: 262.666 sec.\n",
      "Epoch 001: Loss: 0.806, Accuracy: 0 - in: 264.824 sec.\n",
      "  avg_loss: tf.Tensor(0.91456044, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.2924652, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/2.txt  - num events: 3537  - red events: 0\n",
      "  possible anomalies using avg: 614 - using max: 0 - eval_max_loss: 1.282773733139038\n",
      "    max: []\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/2.txt  - num events: 3537  - red events: 0\n",
      "Epoch 000: Loss: 0.681, Accuracy: 0 - in: 116.296 sec.\n",
      "Epoch 001: Loss: 0.587, Accuracy: 0 - in: 115.789 sec.\n",
      "  avg_loss: tf.Tensor(0.6333995, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.8515451, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/3.txt  - num events: 6235  - red events: 0\n",
      "  possible anomalies using avg: 453 - using max: 13 - eval_max_loss: 1.0138059854507446\n",
      "    max: [(560, 1.0138059854507446), (185, 0.9913110733032227), (4145, 0.9468334317207336), (753, 0.879872739315033), (764, 0.879872739315033), (769, 0.879872739315033), (773, 0.879872739315033), (895, 0.879872739315033), (899, 0.879872739315033), (914, 0.879872739315033)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/3.txt  - num events: 6235  - red events: 0\n",
      "Epoch 000: Loss: 0.408, Accuracy: 0 - in: 202.358 sec.\n",
      "Epoch 001: Loss: 0.267, Accuracy: 0 - in: 199.081 sec.\n",
      "  avg_loss: tf.Tensor(0.33478677, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.5372088, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/4.txt  - num events: 4340  - red events: 0\n",
      "  possible anomalies using avg: 544 - using max: 12 - eval_max_loss: 1.0000938177108765\n",
      "    max: [(939, 1.0000938177108765), (3935, 0.9389256834983826), (3937, 0.9389256834983826), (2874, 0.9336707592010498), (4, 0.6138394474983215), (3035, 0.5431104898452759), (3781, 0.5431104898452759), (4330, 0.5431104898452759), (3977, 0.5388160347938538), (3987, 0.5388160347938538)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/4.txt  - num events: 4340  - red events: 0\n",
      "Epoch 000: Loss: 0.199, Accuracy: 0 - in: 137.927 sec.\n",
      "Epoch 001: Loss: 0.136, Accuracy: 0 - in: 145.920 sec.\n",
      "  avg_loss: tf.Tensor(0.16736175, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.39979416, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/5.txt  - num events: 9620  - red events: 0\n",
      "  possible anomalies using avg: 1897 - using max: 47 - eval_max_loss: 1.2482975721359253\n",
      "    max: [(4995, 1.2482975721359253), (5005, 1.2482975721359253), (5006, 1.2482975721359253), (5009, 1.2482975721359253), (5031, 1.2482975721359253), (5044, 1.2482975721359253), (5047, 1.2482975721359253), (3338, 1.1997390985488892), (3346, 1.1997390985488892), (2419, 1.1812878847122192)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/5.txt  - num events: 9620  - red events: 0\n",
      "Epoch 000: Loss: 0.087, Accuracy: 0 - in: 326.489 sec.\n",
      "Epoch 001: Loss: 0.052, Accuracy: 0 - in: 304.213 sec.\n",
      "  avg_loss: tf.Tensor(0.06889845, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.23926193, shape=(), dtype=float32)\n",
      "max_input_lenght: 71\n",
      "  evaluating: data/char_feats/U12/6.txt  - num events: 10716  - red events: 0\n",
      "  possible anomalies using avg: 3840 - using max: 603 - eval_max_loss: 1.7666743993759155\n",
      "    max: [(6785, 1.7666743993759155), (6780, 0.8492916226387024), (3949, 0.8065117001533508), (3567, 0.7410157322883606), (3632, 0.7410157322883606), (3642, 0.7410157322883606), (3649, 0.7410157322883606), (4124, 0.7410157322883606), (4134, 0.7410157322883606), (4149, 0.7410157322883606)]\n",
      "    red events: []\n",
      "max_input_lenght: 71\n",
      "processing: data/char_feats/U12/6.txt  - num events: 10716  - red events: 0\n",
      "Epoch 000: Loss: 0.055, Accuracy: 0 - in: 365.442 sec.\n",
      "Epoch 001: Loss: 0.037, Accuracy: 0 - in: 356.222 sec.\n",
      "  avg_loss: tf.Tensor(0.04589798, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.20779866, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/7.txt  - num events: 7650  - red events: 0\n",
      "  possible anomalies using avg: 2456 - using max: 18 - eval_max_loss: 0.9696515798568726\n",
      "    max: [(5090, 0.9696515798568726), (5095, 0.9696515798568726), (5239, 0.9696515798568726), (3721, 0.5952590703964233), (5820, 0.5952590703964233), (5098, 0.5348685383796692), (5769, 0.5293976664543152), (5771, 0.5293976664543152), (5097, 0.5220378637313843), (5158, 0.5220378637313843)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/7.txt  - num events: 7650  - red events: 0\n",
      "Epoch 000: Loss: 0.026, Accuracy: 0 - in: 249.923 sec.\n",
      "Epoch 001: Loss: 0.018, Accuracy: 0 - in: 247.179 sec.\n",
      "  avg_loss: tf.Tensor(0.022203268, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.05876061, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/8.txt  - num events: 11685  - red events: 0\n",
      "  possible anomalies using avg: 1926 - using max: 232 - eval_max_loss: 0.46465763449668884\n",
      "    max: [(3321, 0.46465763449668884), (5865, 0.46465763449668884), (6479, 0.46465763449668884), (7096, 0.46465763449668884), (4474, 0.3336697816848755), (4475, 0.3336697816848755), (4478, 0.3336697816848755), (4479, 0.3336697816848755), (4480, 0.3336697816848755), (4481, 0.3336697816848755)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/8.txt  - num events: 11685  - red events: 0\n",
      "Epoch 000: Loss: 0.008, Accuracy: 0 - in: 374.887 sec.\n",
      "Epoch 001: Loss: 0.005, Accuracy: 0 - in: 368.557 sec.\n",
      "  avg_loss: tf.Tensor(0.0064365873, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.031701677, shape=(), dtype=float32)\n",
      "max_input_lenght: 64\n",
      "  evaluating: data/char_feats/U12/9.txt  - num events: 7405  - red events: 0\n",
      "  possible anomalies using avg: 1601 - using max: 156 - eval_max_loss: 1.1980857849121094\n",
      "    max: [(4730, 1.1980857849121094), (4921, 1.1980857849121094), (5879, 1.1980857849121094), (5753, 0.49939778447151184), (5371, 0.30882808566093445), (5374, 0.30882808566093445), (5366, 0.24928855895996094), (5368, 0.24928855895996094), (5372, 0.24928855895996094), (5373, 0.24928855895996094)]\n",
      "    red events: []\n",
      "max_input_lenght: 64\n",
      "processing: data/char_feats/U12/9.txt  - num events: 7405  - red events: 0\n",
      "Epoch 000: Loss: 0.004, Accuracy: 0 - in: 235.350 sec.\n",
      "Epoch 001: Loss: 0.003, Accuracy: 0 - in: 238.070 sec.\n",
      "  avg_loss: tf.Tensor(0.003424692, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.017228175, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/10.txt  - num events: 7573  - red events: 0\n",
      "  possible anomalies using avg: 893 - using max: 50 - eval_max_loss: 0.151017427444458\n",
      "    max: [(1064, 0.151017427444458), (3, 0.10394780337810516), (1063, 0.08280602097511292), (3455, 0.04408256709575653), (3478, 0.04408256709575653), (3498, 0.04408256709575653), (3505, 0.04408256709575653), (3503, 0.04216192662715912), (3461, 0.031099600717425346), (3471, 0.031099600717425346)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/10.txt  - num events: 7573  - red events: 0\n",
      "Epoch 000: Loss: 0.001, Accuracy: 0 - in: 238.810 sec.\n",
      "Epoch 001: Loss: 0.001, Accuracy: 0 - in: 236.792 sec.\n",
      "  avg_loss: tf.Tensor(0.00097623194, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.007684809, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/11.txt  - num events: 11563  - red events: 0\n",
      "  possible anomalies using avg: 1001 - using max: 77 - eval_max_loss: 0.12435849756002426\n",
      "    max: [(1086, 0.12435849756002426), (1085, 0.06945713609457016), (3843, 0.031590528786182404), (3845, 0.031590528786182404), (5177, 0.031590528786182404), (5205, 0.031590528786182404), (7675, 0.031590528786182404), (5172, 0.03158603236079216), (5176, 0.03158603236079216), (5178, 0.03158603236079216)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: data/char_feats/U12/11.txt  - num events: 11563  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 371.040 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 362.730 sec.\n",
      "  avg_loss: tf.Tensor(0.00021402424, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0015449403, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/12.txt  - num events: 11422  - red events: 4\n",
      "... score for red event: 0.10726003348827362\n",
      "... score for red event: 0.10455214977264404\n",
      "... score for red event: 0.10455214977264404\n",
      "... score for red event: 0.10455214977264404\n",
      "  possible anomalies using avg: 938 - using max: 142 - eval_max_loss: 0.37292182445526123\n",
      "    max: [(3795, 0.37292182445526123), (4303, 0.2704625427722931), (3, 0.1404290795326233), (29, 0.1404290795326233), (3718, 0.10726003348827362), (3742, 0.10455214977264404), (3861, 0.10455214977264404), (4306, 0.10455214977264404), (1147, 0.08515266329050064), (3837, 0.08341553807258606)]\n",
      "    red events: [3718, 3742, 3861, 4306]\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/12.txt  - num events: 11422  - red events: 4\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 355.840 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 343.420 sec.\n",
      "  avg_loss: tf.Tensor(0.00019377674, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.003870314, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/13.txt  - num events: 7206  - red events: 0\n",
      "  possible anomalies using avg: 644 - using max: 90 - eval_max_loss: 0.4051973223686218\n",
      "    max: [(1057, 0.4051973223686218), (1059, 0.4051973223686218), (4420, 0.12049096077680588), (4428, 0.12049096077680588), (4434, 0.12049096077680588), (4436, 0.12049096077680588), (1062, 0.05840623378753662), (6, 0.05751325190067291), (99, 0.011673212982714176), (120, 0.011673212982714176)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/13.txt  - num events: 7206  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 219.287 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 217.822 sec.\n",
      "  avg_loss: tf.Tensor(0.0002564099, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0039610513, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/14.txt  - num events: 4864  - red events: 0\n",
      "  possible anomalies using avg: 212 - using max: 7 - eval_max_loss: 0.029532385990023613\n",
      "    max: [(1017, 0.029532385990023613), (3232, 0.0284526776522398), (3836, 0.0284526776522398), (3840, 0.0284526776522398), (3907, 0.0284526776522398), (1016, 0.006918500177562237), (3014, 0.006047387141734362)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/14.txt  - num events: 4864  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 147.831 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 146.977 sec.\n",
      "  avg_loss: tf.Tensor(9.107946e-05, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.00083490263, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/15.txt  - num events: 2768  - red events: 0\n",
      "  possible anomalies using avg: 454 - using max: 57 - eval_max_loss: 0.05086958408355713\n",
      "    max: [(3, 0.05086958408355713), (7, 0.05086958408355713), (1192, 0.03647635504603386), (239, 0.023170050233602524), (1293, 0.007928041741251945), (1297, 0.007928041741251945), (1307, 0.007410163059830666), (1016, 0.005816335324198008), (238, 0.0051710037514567375), (649, 0.0033939199056476355)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/15.txt  - num events: 2768  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 83.125 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 82.778 sec.\n",
      "  avg_loss: tf.Tensor(0.00015280004, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0007348322, shape=(), dtype=float32)\n",
      "max_input_lenght: 64\n",
      "  evaluating: data/char_feats/U12/16.txt  - num events: 6093  - red events: 0\n",
      "  possible anomalies using avg: 261 - using max: 27 - eval_max_loss: 1.0949774980545044\n",
      "    max: [(1399, 1.0949774980545044), (1764, 1.0949774980545044), (1694, 0.12569628655910492), (1715, 0.12569628655910492), (1693, 0.11957529932260513), (1696, 0.11957529932260513), (1714, 0.11957529932260513), (4, 0.03222210332751274), (1431, 0.009506404399871826), (230, 0.0088480981066823)]\n",
      "    red events: []\n",
      "max_input_lenght: 64\n",
      "processing: data/char_feats/U12/16.txt  - num events: 6093  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 183.695 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 182.961 sec.\n",
      "  avg_loss: tf.Tensor(0.00034290928, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.006729072, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/17.txt  - num events: 10977  - red events: 0\n",
      "  possible anomalies using avg: 52 - using max: 2 - eval_max_loss: 0.025207480415701866\n",
      "    max: [(2, 0.025207480415701866), (998, 0.016282521188259125)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/17.txt  - num events: 10977  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 327.312 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 325.925 sec.\n",
      "  avg_loss: tf.Tensor(1.0439389e-05, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0002873042, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/18.txt  - num events: 5466  - red events: 0\n",
      "  possible anomalies using avg: 331 - using max: 6 - eval_max_loss: 0.34715381264686584\n",
      "    max: [(3452, 0.34715381264686584), (2, 0.015620877966284752), (12, 0.015620877966284752), (1044, 0.0032660362776368856), (1043, 0.001289720879867673), (3213, 0.00045146376942284405)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/18.txt  - num events: 5466  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 165.393 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 164.138 sec.\n",
      "  avg_loss: tf.Tensor(5.8362144e-05, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0018071976, shape=(), dtype=float32)\n",
      "max_input_lenght: 64\n",
      "  evaluating: data/char_feats/U12/19.txt  - num events: 9631  - red events: 0\n",
      "  possible anomalies using avg: 34 - using max: 14 - eval_max_loss: 0.745854914188385\n",
      "    max: [(4462, 0.745854914188385), (5020, 0.745854914188385), (4515, 0.741607129573822), (4626, 0.495004802942276), (4447, 0.041766125708818436), (4448, 0.03853386640548706), (4526, 0.01958584040403366), (5047, 0.017992950975894928), (3, 0.016303731128573418), (9, 0.016303731128573418)]\n",
      "    red events: []\n",
      "max_input_lenght: 64\n",
      "processing: data/char_feats/U12/19.txt  - num events: 9631  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 289.959 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 288.726 sec.\n",
      "  avg_loss: tf.Tensor(0.0002354817, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.008121737, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/20.txt  - num events: 7859  - red events: 0\n",
      "  possible anomalies using avg: 110 - using max: 8 - eval_max_loss: 0.04323290288448334\n",
      "    max: [(3446, 0.04323290288448334), (3460, 0.04323290288448334), (3467, 0.04323290288448334), (3498, 0.04323290288448334), (3503, 0.04323290288448334), (3511, 0.04323290288448334), (3517, 0.04323290288448334), (14, 0.011120936833322048)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/20.txt  - num events: 7859  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 236.416 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 235.365 sec.\n",
      "  avg_loss: tf.Tensor(3.62981e-05, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0013318359, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/21.txt  - num events: 5189  - red events: 0\n",
      "  possible anomalies using avg: 241 - using max: 15 - eval_max_loss: 0.16998419165611267\n",
      "    max: [(3593, 0.16998419165611267), (4745, 0.16998419165611267), (2, 0.007381714414805174), (9, 0.007381714414805174), (3296, 0.0020869297441095114), (3306, 0.0020869297441095114), (3309, 0.0020869297441095114), (3324, 0.0020869297441095114), (3329, 0.0020869297441095114), (3344, 0.0020869297441095114)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/21.txt  - num events: 5189  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 154.905 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 153.775 sec.\n",
      "  avg_loss: tf.Tensor(6.4932814e-05, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.00069342105, shape=(), dtype=float32)\n",
      "max_input_lenght: 64\n",
      "  evaluating: data/char_feats/U12/22.txt  - num events: 2168  - red events: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  possible anomalies using avg: 41 - using max: 14 - eval_max_loss: 0.47223135828971863\n",
      "    max: [(911, 0.47223135828971863), (1138, 0.47223135828971863), (975, 0.26923179626464844), (1321, 0.26923179626464844), (1316, 0.017774051055312157), (1315, 0.012499010190367699), (1317, 0.012499010190367699), (232, 0.007134323939681053), (7, 0.006148217711597681), (903, 0.004667764995247126)]\n",
      "    red events: []\n",
      "max_input_lenght: 64\n",
      "processing: data/char_feats/U12/22.txt  - num events: 2168  - red events: 0\n",
      "Epoch 000: Loss: 0.001, Accuracy: 0 - in: 65.220 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 64.650 sec.\n",
      "  avg_loss: tf.Tensor(0.0004271939, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0028752035, shape=(), dtype=float32)\n",
      "max_input_lenght: 64\n",
      "  evaluating: data/char_feats/U12/23.txt  - num events: 2244  - red events: 0\n",
      "  possible anomalies using avg: 22 - using max: 17 - eval_max_loss: 0.27868354320526123\n",
      "    max: [(1114, 0.27868354320526123), (1285, 0.27868354320526123), (1369, 0.27868354320526123), (1536, 0.04553462564945221), (747, 0.02511848695576191), (759, 0.02511848695576191), (763, 0.02511848695576191), (767, 0.02511848695576191), (1038, 0.02511848695576191), (1043, 0.02511848695576191)]\n",
      "    red events: []\n",
      "max_input_lenght: 64\n",
      "processing: data/char_feats/U12/23.txt  - num events: 2244  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 69.783 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 69.399 sec.\n",
      "  avg_loss: tf.Tensor(0.00038589642, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0035809618, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/24.txt  - num events: 1715  - red events: 0\n",
      "  possible anomalies using avg: 3 - using max: 2 - eval_max_loss: 0.00848039798438549\n",
      "    max: [(4, 0.00848039798438549), (10, 0.00848039798438549)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/24.txt  - num events: 1715  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 52.052 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 52.246 sec.\n",
      "  avg_loss: tf.Tensor(1.0842823e-05, shape=(), dtype=float32)  - max_loss: tf.Tensor(7.0942624e-05, shape=(), dtype=float32)\n",
      "max_input_lenght: 60\n",
      "  evaluating: data/char_feats/U12/25.txt  - num events: 8321  - red events: 0\n",
      "  possible anomalies using avg: 102 - using max: 15 - eval_max_loss: 0.004614540375769138\n",
      "    max: [(4, 0.004614540375769138), (16, 0.004614540375769138), (3871, 0.0009379074326716363), (3872, 0.0009379074326716363), (3879, 0.0009379074326716363), (3880, 0.0009379074326716363), (204, 0.0006264738622121513), (203, 0.00025963643565773964), (537, 0.0001421494089299813), (1080, 0.00014185924374032766)]\n",
      "    red events: []\n",
      "max_input_lenght: 60\n",
      "processing: data/char_feats/U12/25.txt  - num events: 8321  - red events: 0\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 248.634 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 247.897 sec.\n",
      "  avg_loss: tf.Tensor(1.7244367e-06, shape=(), dtype=float32)  - max_loss: tf.Tensor(4.0508465e-05, shape=(), dtype=float32)\n",
      "max_input_lenght: 64\n",
      "  evaluating: data/char_feats/U12/26.txt  - num events: 5497  - red events: 2\n",
      "... score for red event: 0.03333854302763939\n",
      "... score for red event: 0.03752107545733452\n",
      "  possible anomalies using avg: 620 - using max: 275 - eval_max_loss: 0.27395713329315186\n",
      "    max: [(3106, 0.27395713329315186), (3843, 0.27395713329315186), (3997, 0.27395713329315186), (4080, 0.27395713329315186), (4310, 0.27395713329315186), (4347, 0.27395713329315186), (4600, 0.27395713329315186), (4875, 0.055677369236946106), (4462, 0.03752107545733452), (4460, 0.03333854302763939)]\n",
      "    red events: [4460, 4462]\n",
      "max_input_lenght: 64\n",
      "processing: data/char_feats/U12/26.txt  - num events: 5497  - red events: 2\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0 - in: 165.167 sec.\n",
      "Epoch 001: Loss: 0.000, Accuracy: 0 - in: 163.984 sec.\n",
      "  avg_loss: tf.Tensor(0.0001670512, shape=(), dtype=float32)  - max_loss: tf.Tensor(0.0023044776, shape=(), dtype=float32)\n",
      "max_input_lenght: 64\n",
      "  evaluating: data/char_feats/U12/27.txt  - num events: 8616  - red events: 0\n",
      "  possible anomalies using avg: 65 - using max: 11 - eval_max_loss: 0.10824622958898544\n",
      "    max: [(1338, 0.10824622958898544), (3244, 0.10824622958898544), (1125, 0.04271935299038887), (1409, 0.04271935299038887), (1453, 0.04271935299038887), (2381, 0.04271935299038887), (2704, 0.04271935299038887), (3240, 0.004733268637210131), (3239, 0.0040172613225877285), (3251, 0.0040172613225877285)]\n",
      "    red events: []\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 200\n",
    "\n",
    "# keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "max_len = 120 # length of sentence\n",
    "\n",
    "model = getModel(max_len)\n",
    "\n",
    "for i in range(27):\n",
    "    dataset_fname = data_dir+'{0}.txt'.format(i)\n",
    "    input_data, target_data, red_events = process_file(dataset_fname, max_len)\n",
    "#     print(input_data.shape)\n",
    "#     print(target_data.shape)\n",
    "    print('processing:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "    training_dataset = training_dataset.batch(batch_size)\n",
    "#     print(training_dataset)\n",
    "    train_losses = [];\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg = tfe.metrics.Mean()\n",
    "        epoch_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "        startTime = time.time()\n",
    "        # training using batches of 'batch_size'\n",
    "        for X, y in tfe.Iterator(training_dataset):\n",
    "            grads = grad(model, X, y)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables), \n",
    "                                     global_step=tf.train.get_or_create_global_step())\n",
    "            batch_loss = loss(model, X, y)\n",
    "            epoch_loss_avg(batch_loss) # batch loss\n",
    "            train_losses.append(tf.reduce_mean(batch_loss))\n",
    "#             epoch_accuracy(model(X), y)\n",
    "\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "#         train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: 0 - in: {:.3f} sec.\".format(epoch, \n",
    "                                                                        epoch_loss_avg.result(), \n",
    "#                                                                         epoch_accuracy.result(),\n",
    "                                                                        (time.time()-startTime)))        \n",
    "\n",
    "    avg_loss = tf.reduce_mean(train_losses)\n",
    "    max_loss = tf.reduce_max(train_losses)\n",
    "    print('  avg_loss:', avg_loss, ' - max_loss:', max_loss)\n",
    "    dataset_fname = data_dir+'{0}.txt'.format(i+1)\n",
    "    input_data, target_data, red_events = process_file(dataset_fname, max_len)\n",
    "#     print(input_data.shape)\n",
    "#     print(target_data.shape)\n",
    "    print('  evaluating:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "    eval_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "    eval_dataset = eval_dataset.batch(batch_size)\n",
    "\n",
    "    line_losses = np.array([])\n",
    "\n",
    "    # eval using batches of 'batch_size'\n",
    "    for X, y in tfe.Iterator(eval_dataset):\n",
    "        line_losses = np.append(line_losses, tf.reduce_mean(loss(model, X, y), axis=1))\n",
    "    \n",
    "\n",
    "    eval_max_loss = 0;\n",
    "    possible_anomalies_avg = []\n",
    "    possible_anomalies_max = []\n",
    "    for i, v in enumerate(line_losses):\n",
    "        if v > avg_loss:\n",
    "            possible_anomalies_avg.append((i, v))\n",
    "        if v > max_loss:\n",
    "            possible_anomalies_max.append((i, v))\n",
    "        eval_max_loss = max(eval_max_loss, v)\n",
    "    \n",
    "    for a,b in red_events:\n",
    "        for i, v in enumerate(line_losses):\n",
    "            if a == i:\n",
    "                print('... score for red event:', v)\n",
    "                \n",
    "    possible_anomalies_avg.sort(key=lambda x: x[1], reverse=True)\n",
    "    possible_anomalies_max.sort(key=lambda x: x[1], reverse=True)\n",
    "    print('  possible anomalies using avg:', len(possible_anomalies_avg), \n",
    "          '- using max:', len(possible_anomalies_max),\n",
    "          '- eval_max_loss:', eval_max_loss)\n",
    "#     print('    avg:', possible_anomalies_avg[:10])\n",
    "    print('    max:', possible_anomalies_max[:10])\n",
    "    print('    red events:', [a for a,b in red_events])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "# axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "# axes[1].plot(train_accuracy_results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = 'models/model_lm_bidir_v1.hdfs'\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    model_filepath,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

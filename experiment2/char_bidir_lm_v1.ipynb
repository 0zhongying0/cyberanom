{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Language model to process user events\n",
    "Bidirectional LSTM model with dropout and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "data_dir = '../data/users_feats/U12/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform character-based input into equivalent numerical versions\n",
    "def encode_data(text, num_chars, max_length):\n",
    "    # create empty vessels for one-hot encoded input\n",
    "    X = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    y = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # loop over inputs and tranform and store in X\n",
    "    for i, sentence in enumerate(text):\n",
    "        sentence = '\\t' + sentence + '\\n'\n",
    "        for j, c in enumerate(sentence):\n",
    "            X[i, j, ord(c)] = 1\n",
    "            if j > 0:\n",
    "                # target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                y[i, j - 1, ord(c)] = 1.\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname, max_len):\n",
    "    \"\"\"\n",
    "        process file by extracting sentences data and encode them producing \n",
    "        a set of input and target data for processing by the model\n",
    "        'fname' contains coma separated fields where the last one is the \n",
    "        sentence to be processes\n",
    "    \"\"\"\n",
    "    data = open(fname).read()\n",
    "\n",
    "    text = []\n",
    "    red_events = []\n",
    "    max_text_len = 0\n",
    "    with open(dataset_fname, 'r') as infile:\n",
    "        for i, line in enumerate(infile.readlines()):\n",
    "            line = line.strip().split(',')\n",
    "            text.append(line[-1])\n",
    "            max_text_len = max(max_text_len, int(line[-2]))\n",
    "            if int(line[2]) == 1:\n",
    "                red_events.append((i,line))\n",
    "\n",
    "#     print(text[0], 'len:', len(text[0]), len(text))\n",
    "    print('max_input_lenght:', max_text_len)\n",
    "    input_data, target_data = encode_data(text, 128, max_len)\n",
    "    \n",
    "    return input_data, target_data, red_events\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model using Keras\n",
    "\n",
    "The TensorFlow [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together. See the [Keras documentation](https://keras.io/) for details.\n",
    "\n",
    "The [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, one [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) and one [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers with 'num_chars' nodes each. The first layer's `input_shape` parameter corresponds to the amount of features from the dataset, and is required.\n",
    "\n",
    "#### Define the loss and gradient function\n",
    "\n",
    "Both training and evaluation stages need to calculate the model's *[loss](https://developers.google.com/machine-learning/crash-course/glossary#loss)*. This measures how off a model's predictions are from the desired output. We want to minimize, or optimize, this value.\n",
    "\n",
    "Our model will calculate its loss using the [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) function which takes the model's prediction and the desired output. The returned loss value is progressively larger as the prediction gets worse.\n",
    "\n",
    "The `grad` function uses the `loss` function and the [tfe.GradientTape](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/GradientTape) to record operations that compute the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = 128 # our vocabulary, i.e. unique characters in text. We'll just use the first 128 (half ASCII)\n",
    "\n",
    "def getModel(max_len):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0., input_shape=(None, num_chars)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(max_len, return_sequences=True)),  # input shape required\n",
    "#         tf.keras.layers.Dense(240, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_chars, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.keras.losses.categorical_crossentropy(y, y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer\n",
    "TensorFlow has many [optimization algorithms](https://www.tensorflow.org/api_guides/python/train) available for training. This model uses the [tf.train.GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) that implements the *[stochastic gradient descent](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)* (SGD) algorithm. The `learning_rate` sets the step size to take for each iteration down the hill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, epsilon=1e-08)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 500\n",
    "\n",
    "# keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "max_len = 120 # length of sentence\n",
    "\n",
    "model = getModel(max_len)\n",
    "\n",
    "for i in range(27):\n",
    "    dataset_fname = data_dir+'{0}.txt'.format(i)\n",
    "    input_data, target_data, red_events = process_file(dataset_fname, max_len)\n",
    "#     print(input_data.shape)\n",
    "#     print(target_data.shape)\n",
    "    print('processing:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "    training_dataset = training_dataset.batch(batch_size)\n",
    "#     print(training_dataset)\n",
    "    train_losses = [];\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg = tfe.metrics.Mean()\n",
    "        epoch_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "        startTime = time.time()\n",
    "        # training using batches of 'batch_size'\n",
    "        for X, y in tfe.Iterator(training_dataset):\n",
    "            grads = grad(model, X, y)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables), \n",
    "                                     global_step=tf.train.get_or_create_global_step())\n",
    "            batch_loss = loss(model, X, y)\n",
    "            epoch_loss_avg(batch_loss) # batch loss\n",
    "            train_losses.append(tf.reduce_mean(batch_loss))\n",
    "#             epoch_accuracy(model(X), y)\n",
    "\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "#         train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: 0 - in: {:.3f} sec.\".format(epoch, \n",
    "                                                                        epoch_loss_avg.result(), \n",
    "#                                                                         epoch_accuracy.result(),\n",
    "                                                                        (time.time()-startTime)))        \n",
    "\n",
    "    avg_loss = tf.reduce_mean(train_losses)\n",
    "    max_loss = tf.reduce_max(train_losses)\n",
    "    print('  avg_loss:', avg_loss, ' - max_loss:', max_loss)\n",
    "    dataset_fname = data_dir+'{0}.txt'.format(i+1)\n",
    "    input_data, target_data, red_events = process_file(dataset_fname, max_len)\n",
    "#     print(input_data.shape)\n",
    "#     print(target_data.shape)\n",
    "    print('  evaluating:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "    eval_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "    eval_dataset = eval_dataset.batch(batch_size)\n",
    "\n",
    "    line_losses = np.array([])\n",
    "\n",
    "    # eval using batches of 'batch_size'\n",
    "    for X, y in tfe.Iterator(eval_dataset):\n",
    "        line_losses = np.append(line_losses, tf.reduce_mean(loss(model, X, y), axis=1))\n",
    "    \n",
    "\n",
    "    eval_max_loss = 0;\n",
    "    possible_anomalies_avg = []\n",
    "    possible_anomalies_max = []\n",
    "    for i, v in enumerate(line_losses):\n",
    "        if v > avg_loss:\n",
    "            possible_anomalies_avg.append((i, v))\n",
    "        if v > max_loss:\n",
    "            possible_anomalies_max.append((i, v))\n",
    "        eval_max_loss = max(eval_max_loss, v)\n",
    "    \n",
    "    for a,b in red_events:\n",
    "        for i, v in enumerate(line_losses):\n",
    "            if a == i:\n",
    "                print('... score for red event:', v)\n",
    "                \n",
    "    possible_anomalies_avg.sort(key=lambda x: x[1], reverse=True)\n",
    "    possible_anomalies_max.sort(key=lambda x: x[1], reverse=True)\n",
    "    print('  possible anomalies using avg:', len(possible_anomalies_avg), \n",
    "          '- using max:', len(possible_anomalies_max),\n",
    "          '- eval_max_loss:', eval_max_loss)\n",
    "#     print('    avg:', possible_anomalies_avg[:10])\n",
    "    print('    max:', possible_anomalies_max[:10])\n",
    "    print('    red events:', [a for a,b in red_events])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "# axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "# axes[1].plot(train_accuracy_results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = 'models/model_lm_bidir_v1.hdfs'\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    model_filepath,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

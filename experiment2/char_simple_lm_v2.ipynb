{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Language model to process user events\n",
    "Normal LSTM based model to evaluate the results of processing users\n",
    "We are using tensorflow eager mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_names = ['U12', 'U13', 'U24', 'U78', 'U207', 'U293', 'U453', 'U679', 'U1289', 'U1480']\n",
    "users_indir = '../data/users_feats'\n",
    "users_lossdir = '../data/users_loss'\n",
    "max_len = 120 # max length of sentence\n",
    "num_chars = 128 # our vocabulary, i.e. unique characters in text. We'll just use the first 128 (half ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform character-based input into equivalent numerical versions\n",
    "def encode_data(text, num_chars, max_length):\n",
    "    # create empty vessels for one-hot encoded input\n",
    "    X = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    y = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # loop over inputs and tranform and store in X\n",
    "    for i, sentence in enumerate(text):\n",
    "        sentence = '\\t' + sentence + '\\n'\n",
    "        for j, c in enumerate(sentence):\n",
    "            X[i, j, ord(c)] = 1\n",
    "            if j > 0:\n",
    "                # target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                y[i, j - 1, ord(c)] = 1.\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(fname):\n",
    "    \"\"\"\n",
    "        process file by extracting sentences data and encode them producing \n",
    "        a set of input and target data for processing by the model\n",
    "        'fname' contains coma separated fields where the last one is the \n",
    "        sentence to be processes\n",
    "    \"\"\"\n",
    "    data = open(fname).read()\n",
    "\n",
    "    text = []\n",
    "    red_events = []\n",
    "    with open(fname, 'r') as infile:\n",
    "        for line in infile.readlines():\n",
    "            line = line.strip().split(',')\n",
    "            text.append(line[-1])\n",
    "            if int(line[2]) == 1:\n",
    "                red_events.append(line)\n",
    "\n",
    "#     print(text[0], 'len:', len(text[0]), len(text))\n",
    "\n",
    "    input_data, target_data = encode_data(text, num_chars, max_len)\n",
    "    \n",
    "    return input_data, target_data, red_events\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model using Keras\n",
    "\n",
    "The TensorFlow [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together. See the [Keras documentation](https://keras.io/) for details.\n",
    "\n",
    "The [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, one [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) and one [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers with 'num_chars' nodes each. The first layer's `input_shape` parameter corresponds to the amount of features from the dataset, and is required.\n",
    "\n",
    "Our model will calculate its loss using the [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) function which takes the model's prediction and the desired output. The returned loss value is progressively larger as the prediction gets worse.\n",
    "\n",
    "The `grad` function uses the `loss` function and the [tfe.GradientTape](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/GradientTape) to record operations that compute the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getModel():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(max_len, input_shape=(None, num_chars), return_sequences=True),  # input shape required\n",
    "#         tf.keras.layers.Dense(num_chars, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_chars, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.keras.losses.categorical_crossentropy(y, y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer\n",
    "This model uses the [tf.train.RMSPropOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, training_dataset, num_epochs):\n",
    "    \n",
    "    train_losses = []\n",
    "    loss_results = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg = tfe.metrics.Mean()\n",
    "\n",
    "        startTime = time.time()\n",
    "        # training using batches of 'batch_size'\n",
    "        for X, y in tfe.Iterator(training_dataset):\n",
    "            grads = grad(model, X, y)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables), \n",
    "                                     global_step=tf.train.get_or_create_global_step())\n",
    "            batch_loss = loss(model, X, y)\n",
    "            epoch_loss_avg(batch_loss) # batch loss\n",
    "            train_losses.append(tf.reduce_mean(batch_loss))\n",
    "\n",
    "        loss_results.append(epoch_loss_avg.result())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch {:03d}: Loss: {:.3f} - in: {:.3f} sec.\".format(epoch, \n",
    "                                                                        epoch_loss_avg.result(), \n",
    "                                                                        (time.time()-startTime)))        \n",
    "\n",
    "    avg_loss = tf.reduce_mean(train_losses)\n",
    "    max_loss = tf.reduce_max(train_losses)\n",
    "    print('  avg_loss:', avg_loss, ' - max_loss:', max_loss)\n",
    " \n",
    "    return loss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_user(user_name, outfile):\n",
    "    num_epochs = 2\n",
    "    batch_size = 500\n",
    "    data_dir = '{0}/{1}/'.format(users_indir, u)\n",
    "    \n",
    "    # keep results for plotting\n",
    "    train_loss_results = []\n",
    "\n",
    "    model = getModel()\n",
    "\n",
    "#     for i in range(57):\n",
    "    for d in range(2):\n",
    "\n",
    "        # training phase\n",
    "        dataset_fname = data_dir+'{0}.txt'.format(d)\n",
    "        print('df:', dataset_fname)\n",
    "        input_data, target_data, red_events = process_file(dataset_fname)\n",
    "        print('processing:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "        training_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "        training_dataset = training_dataset.batch(batch_size)\n",
    "\n",
    "        # train model on a day\n",
    "        loss_results = train(model, training_dataset, num_epochs)\n",
    "        train_loss_results.append(loss_results)\n",
    "\n",
    "        # evaluation phase\n",
    "        dataset_fname = data_dir+'{0}.txt'.format(d+1)\n",
    "        input_data, target_data, red_events = process_file(dataset_fname)\n",
    "        print('  evaluating:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "        eval_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "        eval_dataset = eval_dataset.batch(batch_size)\n",
    "\n",
    "        line_losses = np.array([])\n",
    "\n",
    "        # eval using batches of 'batch_size'\n",
    "        for X, y in tfe.Iterator(eval_dataset):\n",
    "            line_losses = np.append(line_losses, tf.reduce_mean(loss(model, X, y), axis=1))\n",
    "\n",
    "        possible_anomalies = [(i,v) for i, v in enumerate(line_losses)]\n",
    "        possible_anomalies.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print('    max:', possible_anomalies[:10])\n",
    "        print('    red events:', [a for a,b in red_events])\n",
    "        \n",
    "        # write top 10 losses to a file with the format (day, score, redevent)\n",
    "        for i, v in possible_anomalies[:20]:\n",
    "            red = '0'\n",
    "            for a,b in red_events:\n",
    "                if a == i:\n",
    "                    red = '1'\n",
    "                    break\n",
    "            line = '{0},{1},{2}\\n'.format(d, v, red)\n",
    "            outfile.write(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files for User: U12\n",
      "df: ../data/users_feats/U12/0.txt\n",
      "processing: ../data/users_feats/U12/0.txt  - num events: 3128  - red events: 0\n",
      "Epoch 000: Loss: 2.053 - in: 71.426 sec.\n",
      "Epoch 001: Loss: 1.665 - in: 71.683 sec.\n",
      "  avg_loss: tf.Tensor(1.8399007, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.1780102, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U12/1.txt  - num events: 8355  - red events: 0\n",
      "    max: [(776, 1.825667142868042), (780, 1.825667142868042), (789, 1.825667142868042), (790, 1.825667142868042), (793, 1.825667142868042), (796, 1.825667142868042), (798, 1.825667142868042), (800, 1.825667142868042), (811, 1.825667142868042), (814, 1.825667142868042)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U12/1.txt\n",
      "processing: ../data/users_feats/U12/1.txt  - num events: 8355  - red events: 0\n",
      "Epoch 000: Loss: 1.485 - in: 175.489 sec.\n",
      "Epoch 001: Loss: 1.389 - in: 187.514 sec.\n",
      "  avg_loss: tf.Tensor(1.435772, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.5778865, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U12/2.txt  - num events: 3537  - red events: 0\n",
      "    max: [(1927, 1.6577606201171875), (2049, 1.6577606201171875), (2054, 1.6577606201171875), (2056, 1.6577606201171875), (2059, 1.6577606201171875), (2061, 1.6577606201171875), (2068, 1.6577606201171875), (2069, 1.6577606201171875), (2071, 1.6577606201171875), (2078, 1.6577606201171875)]\n",
      "    red events: []\n",
      "Processing files for User: U13\n",
      "df: ../data/users_feats/U13/0.txt\n",
      "processing: ../data/users_feats/U13/0.txt  - num events: 22158  - red events: 0\n",
      "Epoch 000: Loss: 1.556 - in: 465.404 sec.\n",
      "Epoch 001: Loss: 1.263 - in: 401.081 sec.\n",
      "  avg_loss: tf.Tensor(1.4066224, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.1687331, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U13/1.txt  - num events: 22312  - red events: 0\n",
      "    max: [(6492, 1.7259193658828735), (11943, 1.7259193658828735), (14787, 1.7259193658828735), (16094, 1.7259193658828735), (344, 1.5548546314239502), (1983, 1.5548546314239502), (2009, 1.5548546314239502), (3498, 1.5548546314239502), (4823, 1.5548546314239502), (6214, 1.5548546314239502)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U13/1.txt\n",
      "processing: ../data/users_feats/U13/1.txt  - num events: 22312  - red events: 0\n",
      "Epoch 000: Loss: 1.108 - in: 407.528 sec.\n",
      "Epoch 001: Loss: 0.968 - in: 403.355 sec.\n",
      "  avg_loss: tf.Tensor(1.0367788, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.2326478, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U13/2.txt  - num events: 14298  - red events: 0\n",
      "    max: [(4954, 1.6206634044647217), (5162, 1.6206634044647217), (7200, 1.6206634044647217), (8135, 1.6206634044647217), (9223, 1.6206634044647217), (1732, 1.2871180772781372), (5004, 1.2802884578704834), (5082, 1.2802884578704834), (5203, 1.2802884578704834), (5273, 1.2802884578704834)]\n",
      "    red events: []\n",
      "Processing files for User: U24\n",
      "df: ../data/users_feats/U24/0.txt\n",
      "processing: ../data/users_feats/U24/0.txt  - num events: 18815  - red events: 0\n",
      "Epoch 000: Loss: 1.605 - in: 343.149 sec.\n",
      "Epoch 001: Loss: 1.345 - in: 339.207 sec.\n",
      "  avg_loss: tf.Tensor(1.4740704, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.1778653, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U24/1.txt  - num events: 19127  - red events: 0\n",
      "    max: [(7592, 2.2812907695770264), (7639, 2.2812907695770264), (9147, 2.2812907695770264), (9150, 2.2812907695770264), (11035, 2.2812907695770264), (5444, 2.227196455001831), (5488, 2.227196455001831), (5495, 2.227196455001831), (5542, 2.227196455001831), (5574, 2.227196455001831)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U24/1.txt\n",
      "processing: ../data/users_feats/U24/1.txt  - num events: 19127  - red events: 0\n",
      "Epoch 000: Loss: 1.229 - in: 351.945 sec.\n",
      "Epoch 001: Loss: 1.112 - in: 345.807 sec.\n",
      "  avg_loss: tf.Tensor(1.1696515, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.4002013, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U24/2.txt  - num events: 20565  - red events: 0\n",
      "    max: [(6529, 2.246147632598877), (6563, 2.246147632598877), (6571, 2.246147632598877), (6887, 2.246147632598877), (6892, 2.246147632598877), (6928, 2.246147632598877), (6936, 2.246147632598877), (7699, 2.246147632598877), (7702, 2.246147632598877), (7877, 2.246147632598877)]\n",
      "    red events: []\n",
      "Processing files for User: U78\n",
      "df: ../data/users_feats/U78/0.txt\n",
      "processing: ../data/users_feats/U78/0.txt  - num events: 25286  - red events: 0\n",
      "Epoch 000: Loss: 1.441 - in: 461.146 sec.\n",
      "Epoch 001: Loss: 1.171 - in: 456.290 sec.\n",
      "  avg_loss: tf.Tensor(1.3045981, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.0356014, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U78/1.txt  - num events: 22875  - red events: 0\n",
      "    max: [(2191, 2.133450508117676), (1655, 2.0299832820892334), (1837, 1.9379022121429443), (15004, 1.9379022121429443), (2019, 1.93593168258667), (2190, 1.8934063911437988), (8, 1.841705083847046), (13, 1.841705083847046), (20, 1.841705083847046), (25, 1.841705083847046)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U78/1.txt\n",
      "processing: ../data/users_feats/U78/1.txt  - num events: 22875  - red events: 0\n",
      "Epoch 000: Loss: 1.040 - in: 416.744 sec.\n",
      "Epoch 001: Loss: 0.906 - in: 411.555 sec.\n",
      "  avg_loss: tf.Tensor(0.97232634, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.1571575, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U78/2.txt  - num events: 22753  - red events: 0\n",
      "    max: [(2550, 1.6593434810638428), (10, 1.5644762516021729), (23, 1.5644762516021729), (30, 1.5644762516021729), (46, 1.5644762516021729), (56, 1.5644762516021729), (64, 1.5644762516021729), (70, 1.5644762516021729), (86, 1.5644762516021729), (93, 1.5644762516021729)]\n",
      "    red events: []\n",
      "Processing files for User: U207\n",
      "df: ../data/users_feats/U207/0.txt\n",
      "processing: ../data/users_feats/U207/0.txt  - num events: 3950  - red events: 0\n",
      "Epoch 000: Loss: 1.696 - in: 71.830 sec.\n",
      "Epoch 001: Loss: 1.406 - in: 70.285 sec.\n",
      "  avg_loss: tf.Tensor(1.5487189, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.8754587, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U207/1.txt  - num events: 3880  - red events: 0\n",
      "    max: [(2303, 2.3262197971343994), (2369, 2.3262197971343994), (1211, 2.2559990882873535), (1268, 2.2559990882873535), (307, 2.1953461170196533), (2235, 2.0677671432495117), (2237, 2.0677671432495117), (2238, 2.0677671432495117), (82, 2.066786766052246), (86, 2.066786766052246)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U207/1.txt\n",
      "processing: ../data/users_feats/U207/1.txt  - num events: 3880  - red events: 0\n",
      "Epoch 000: Loss: 1.335 - in: 71.811 sec.\n",
      "Epoch 001: Loss: 1.307 - in: 69.335 sec.\n",
      "  avg_loss: tf.Tensor(1.3191779, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.3962991, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U207/2.txt  - num events: 3657  - red events: 0\n",
      "    max: [(1642, 2.2161865234375), (269, 2.1476545333862305), (1403, 1.9729560613632202), (2291, 1.9729560613632202), (2348, 1.9729560613632202), (2350, 1.9729560613632202), (2351, 1.9729560613632202), (2352, 1.9729560613632202), (2355, 1.9729560613632202), (2359, 1.9729560613632202)]\n",
      "    red events: []\n",
      "Processing files for User: U293\n",
      "df: ../data/users_feats/U293/0.txt\n",
      "processing: ../data/users_feats/U293/0.txt  - num events: 2404  - red events: 0\n",
      "Epoch 000: Loss: 2.196 - in: 44.054 sec.\n",
      "Epoch 001: Loss: 1.770 - in: 43.200 sec.\n",
      "  avg_loss: tf.Tensor(1.9777635, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.2771823, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U293/1.txt  - num events: 3838  - red events: 0\n",
      "    max: [(1659, 2.466108798980713), (2664, 2.2342028617858887), (3300, 2.2342028617858887), (3429, 2.2342028617858887), (2892, 2.2133538722991943), (3318, 2.2133538722991943), (3350, 2.156428575515747), (557, 2.0163614749908447), (721, 2.0163614749908447), (977, 2.0163614749908447)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U293/1.txt\n",
      "processing: ../data/users_feats/U293/1.txt  - num events: 3838  - red events: 0\n",
      "Epoch 000: Loss: 1.644 - in: 70.425 sec.\n",
      "Epoch 001: Loss: 1.587 - in: 69.017 sec.\n",
      "  avg_loss: tf.Tensor(1.6136312, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.6902859, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U293/2.txt  - num events: 2904  - red events: 0\n",
      "    max: [(1312, 2.0913517475128174), (676, 1.8982523679733276), (811, 1.8982523679733276), (930, 1.8982523679733276), (1034, 1.8982523679733276), (1266, 1.8982523679733276), (1416, 1.8982523679733276), (1540, 1.8982523679733276), (1689, 1.8982523679733276), (1766, 1.8982523679733276)]\n",
      "    red events: []\n",
      "Processing files for User: U453\n",
      "df: ../data/users_feats/U453/0.txt\n",
      "processing: ../data/users_feats/U453/0.txt  - num events: 8756  - red events: 0\n",
      "Epoch 000: Loss: 1.893 - in: 160.288 sec.\n",
      "Epoch 001: Loss: 1.604 - in: 157.474 sec.\n",
      "  avg_loss: tf.Tensor(1.7444482, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.341828, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U453/1.txt  - num events: 6593  - red events: 0\n",
      "    max: [(574, 2.4492132663726807), (2851, 2.4082188606262207), (3982, 2.401089906692505), (582, 2.172173500061035), (570, 1.8312716484069824), (2850, 1.764715552330017), (149, 1.7540600299835205), (3981, 1.7539598941802979), (861, 1.7459849119186401), (3054, 1.7459849119186401)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U453/1.txt\n",
      "processing: ../data/users_feats/U453/1.txt  - num events: 6593  - red events: 0\n",
      "Epoch 000: Loss: 1.468 - in: 121.874 sec.\n",
      "Epoch 001: Loss: 1.423 - in: 119.076 sec.\n",
      "  avg_loss: tf.Tensor(1.4446627, shape=(), dtype=float32)  - max_loss: tf.Tensor(1.5291955, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U453/2.txt  - num events: 686  - red events: 0\n",
      "    max: [(228, 2.1096911430358887), (332, 1.9392898082733154), (34, 1.6419121026992798), (39, 1.6419121026992798), (66, 1.6419121026992798), (96, 1.6419121026992798), (102, 1.6419121026992798), (127, 1.6419121026992798), (128, 1.6419121026992798), (134, 1.6419121026992798)]\n",
      "    red events: []\n",
      "Processing files for User: U679\n",
      "df: ../data/users_feats/U679/0.txt\n",
      "processing: ../data/users_feats/U679/0.txt  - num events: 860  - red events: 0\n",
      "Epoch 000: Loss: 2.276 - in: 7211.982 sec.\n",
      "Epoch 001: Loss: 2.232 - in: 16.869 sec.\n",
      "  avg_loss: tf.Tensor(2.2518122, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.2805865, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U679/1.txt  - num events: 1015  - red events: 0\n",
      "    max: [(550, 2.4171128273010254), (554, 2.4171128273010254), (563, 2.4171128273010254), (613, 2.4171128273010254), (623, 2.4171128273010254), (673, 2.4171128273010254), (678, 2.4171128273010254), (693, 2.4171128273010254), (694, 2.4171128273010254), (763, 2.4171128273010254)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U679/1.txt\n",
      "processing: ../data/users_feats/U679/1.txt  - num events: 1015  - red events: 0\n",
      "Epoch 000: Loss: 1.963 - in: 20.343 sec.\n",
      "Epoch 001: Loss: 1.786 - in: 7216.744 sec.\n",
      "  avg_loss: tf.Tensor(1.864776, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.033175, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U679/2.txt  - num events: 943  - red events: 0\n",
      "    max: [(849, 1.9817161560058594), (936, 1.9817161560058594), (21, 1.9786908626556396), (27, 1.9786908626556396), (112, 1.9786908626556396), (134, 1.9786908626556396), (169, 1.9786908626556396), (177, 1.9786908626556396), (221, 1.9786908626556396), (224, 1.9786908626556396)]\n",
      "    red events: []\n",
      "Processing files for User: U1289\n",
      "df: ../data/users_feats/U1289/0.txt\n",
      "processing: ../data/users_feats/U1289/0.txt  - num events: 932  - red events: 0\n",
      "Epoch 000: Loss: 2.305 - in: 17.171 sec.\n",
      "Epoch 001: Loss: 2.240 - in: 7212.556 sec.\n",
      "  avg_loss: tf.Tensor(2.2742343, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.3410232, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U1289/1.txt  - num events: 1728  - red events: 0\n",
      "    max: [(851, 2.7437009811401367), (855, 2.7437009811401367), (494, 2.6624302864074707), (498, 2.6624302864074707), (598, 2.6624302864074707), (605, 2.6624302864074707), (766, 2.6624302864074707), (774, 2.6624302864074707), (819, 2.6624302864074707), (821, 2.6624302864074707)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U1289/1.txt\n",
      "processing: ../data/users_feats/U1289/1.txt  - num events: 1728  - red events: 0\n",
      "Epoch 000: Loss: 1.895 - in: 33.025 sec.\n",
      "Epoch 001: Loss: 1.736 - in: 7228.573 sec.\n",
      "  avg_loss: tf.Tensor(1.8156562, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.036651, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U1289/2.txt  - num events: 1137  - red events: 0\n",
      "    max: [(273, 2.008190870285034), (285, 2.008190870285034), (347, 2.008190870285034), (349, 2.008190870285034), (448, 2.008190870285034), (449, 2.008190870285034), (551, 2.008190870285034), (556, 2.008190870285034), (568, 2.008190870285034), (569, 2.008190870285034)]\n",
      "    red events: []\n",
      "Processing files for User: U1480\n",
      "df: ../data/users_feats/U1480/0.txt\n",
      "processing: ../data/users_feats/U1480/0.txt  - num events: 629  - red events: 0\n",
      "Epoch 000: Loss: 2.296 - in: 13.394 sec.\n",
      "Epoch 001: Loss: 2.275 - in: 7208.648 sec.\n",
      "  avg_loss: tf.Tensor(2.2979352, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.3306391, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U1480/1.txt  - num events: 497  - red events: 0\n",
      "    max: [(250, 2.6253697872161865), (262, 2.6253697872161865), (493, 2.6253697872161865), (165, 2.6188294887542725), (177, 2.6188294887542725), (0, 2.618241310119629), (5, 2.618241310119629), (15, 2.618241310119629), (21, 2.618241310119629), (26, 2.618241310119629)]\n",
      "    red events: []\n",
      "df: ../data/users_feats/U1480/1.txt\n",
      "processing: ../data/users_feats/U1480/1.txt  - num events: 497  - red events: 0\n",
      "Epoch 000: Loss: 2.237 - in: 9.294 sec.\n",
      "Epoch 001: Loss: 2.080 - in: 9.136 sec.\n",
      "  avg_loss: tf.Tensor(2.1582599, shape=(), dtype=float32)  - max_loss: tf.Tensor(2.2366545, shape=(), dtype=float32)\n",
      "  evaluating: ../data/users_feats/U1480/2.txt  - num events: 793  - red events: 0\n",
      "    max: [(297, 2.401575803756714), (334, 2.401575803756714), (708, 2.401575803756714), (246, 2.38397216796875), (54, 2.3695473670959473), (91, 2.3695473670959473), (131, 2.3695473670959473), (184, 2.3695473670959473), (231, 2.3695473670959473), (290, 2.3695473670959473)]\n",
      "    red events: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists(users_lossdir):\n",
    "    os.makedirs(users_lossdir)\n",
    "    \n",
    "for u in user_names:\n",
    "    print('Processing files for User:', u)\n",
    "    outfile_name = \"{0}/{1}_losses.txt\".format(users_lossdir, u)\n",
    "\n",
    "    with open(outfile_name, 'w') as outfile:\n",
    "        outfile.write('day,loss,redevent\\n')\n",
    "        process_user(u, outfile)\n",
    "        outfile.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "# axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "# axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "# axes[1].plot(train_accuracy_results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_filepath = 'models/model_lm_v1.hdfs'\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    model_filepath,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

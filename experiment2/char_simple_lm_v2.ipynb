{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Language model to process user events\n",
    "Normal LSTM based model to evaluate the results of processing users\n",
    "We are using tensorflow eager mode\n",
    "This notebook process all files for users in the 'user_names' list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_names = ['U12', 'U13', 'U24', 'U78', 'U207', 'U293', 'U453', 'U679', 'U1289', 'U1480']\n",
    "users_indir = '../data/users_feats'\n",
    "users_lossdir = '../data/users_loss'\n",
    "users_modeldir = '../data/users_model'\n",
    "\n",
    "max_len = 120 # max length of sentence\n",
    "num_chars = 128 # our vocabulary, i.e. unique characters in text. We'll just use the first 128 (half ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform character-based input into equivalent numerical versions\n",
    "def encode_data(text, num_chars, max_length):\n",
    "    # create empty vessels for one-hot encoded input\n",
    "    X = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    y = np.zeros((len(text), max_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # loop over inputs and tranform and store in X\n",
    "    for i, sentence in enumerate(text):\n",
    "        sentence = '\\t' + sentence + '\\n'\n",
    "        for j, c in enumerate(sentence):\n",
    "            X[i, j, ord(c)] = 1\n",
    "            if j > 0:\n",
    "                # target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                y[i, j - 1, ord(c)] = 1.\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(fname):\n",
    "    \"\"\"\n",
    "        process file by extracting sentences data and encode them producing \n",
    "        a set of input and target data for processing by the model\n",
    "        'fname' contains coma separated fields where the last one is the \n",
    "        sentence to be processes\n",
    "    \"\"\"\n",
    "    data = open(fname).read()\n",
    "\n",
    "    text = []\n",
    "    red_events = []\n",
    "    with open(fname, 'r') as infile:\n",
    "        for i, line in enumerate(infile.readlines()):\n",
    "            line = line.strip().split(',')\n",
    "            text.append(line[-1])\n",
    "            if int(line[2]) == 1:\n",
    "                red_events.append((i,line))\n",
    "\n",
    "#     print(text[0], 'len:', len(text[0]), len(text))\n",
    "\n",
    "    input_data, target_data = encode_data(text, num_chars, max_len)\n",
    "    \n",
    "    return input_data, target_data, red_events\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model using Keras\n",
    "\n",
    "The TensorFlow [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together. See the [Keras documentation](https://keras.io/) for details.\n",
    "\n",
    "The [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, one [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) and one [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers with 'num_chars' nodes each. The first layer's `input_shape` parameter corresponds to the amount of features from the dataset, and is required.\n",
    "\n",
    "Our model will calculate its loss using the [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) function which takes the model's prediction and the desired output. The returned loss value is progressively larger as the prediction gets worse.\n",
    "\n",
    "The `grad` function uses the `loss` function and the [tfe.GradientTape](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/GradientTape) to record operations that compute the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getModel():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(max_len, input_shape=(None, num_chars), return_sequences=True),  # input shape required\n",
    "#         tf.keras.layers.Dense(num_chars, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_chars, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.keras.losses.categorical_crossentropy(y, y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer\n",
    "This model uses the [tf.train.RMSPropOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, training_dataset, num_epochs):\n",
    "    \n",
    "    train_losses = []\n",
    "    loss_results = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg = tfe.metrics.Mean()\n",
    "\n",
    "        startTime = time.time()\n",
    "        # training using batches of 'batch_size'\n",
    "        for X, y in tfe.Iterator(training_dataset):\n",
    "            grads = grad(model, X, y)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables), \n",
    "                                     global_step=tf.train.get_or_create_global_step())\n",
    "            batch_loss = loss(model, X, y)\n",
    "            epoch_loss_avg(batch_loss) # batch loss\n",
    "            train_losses.append(tf.reduce_mean(batch_loss))\n",
    "\n",
    "        loss_results.append(epoch_loss_avg.result())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch {:03d}: Loss: {:.3f} - in: {:.3f} sec.\".format(epoch, \n",
    "                                                                        epoch_loss_avg.result(), \n",
    "                                                                        (time.time()-startTime)))        \n",
    "\n",
    "    avg_loss = tf.reduce_mean(train_losses)\n",
    "    max_loss = tf.reduce_max(train_losses)\n",
    "    print('  avg_loss:', avg_loss, ' - max_loss:', max_loss)\n",
    " \n",
    "    return loss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_user(user_name, outfile):\n",
    "    num_epochs = 2\n",
    "    batch_size = 500\n",
    "    data_dir = '{0}/{1}/'.format(users_indir, u)\n",
    "    \n",
    "    # keep results for plotting\n",
    "    train_loss_results = []\n",
    "\n",
    "    model = getModel()\n",
    "\n",
    "    for d in range(57):\n",
    "\n",
    "        # training phase\n",
    "        dataset_fname = data_dir+'{0}.txt'.format(d)\n",
    "        print('df:', dataset_fname)\n",
    "        input_data, target_data, red_events = process_file(dataset_fname)\n",
    "        print('processing:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "        training_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "        training_dataset = training_dataset.batch(batch_size)\n",
    "\n",
    "        # train model on a day\n",
    "        loss_results = train(model, training_dataset, num_epochs)\n",
    "        train_loss_results.append(loss_results)\n",
    "\n",
    "        # some cleanup\n",
    "        del input_data\n",
    "        del target_data\n",
    "\n",
    "        \n",
    "        # evaluation phase\n",
    "        dataset_fname = data_dir+'{0}.txt'.format(d+1)\n",
    "        input_data, target_data, red_events = process_file(dataset_fname)\n",
    "        print('  evaluating:', dataset_fname, \" - num events:\", len(input_data), \" - red events:\", len(red_events))\n",
    "\n",
    "        eval_dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n",
    "        eval_dataset = eval_dataset.batch(batch_size)\n",
    "\n",
    "        line_losses = np.array([])\n",
    "        \n",
    "        # eval using batches of 'batch_size'\n",
    "        for X, y in tfe.Iterator(eval_dataset):\n",
    "            line_losses = np.append(line_losses, tf.reduce_mean(loss(model, X, y), axis=1))\n",
    "\n",
    "        # some cleanup\n",
    "        del input_data\n",
    "        del target_data\n",
    "\n",
    "        possible_anomalies = [(i,v) for i, v in enumerate(line_losses)]\n",
    "        possible_anomalies.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print('    max:', possible_anomalies[:10])\n",
    "        print('    red events:', [a for a,b in red_events])\n",
    "        \n",
    "        # write top 10 losses to a file with the format (day, score, redevent)\n",
    "        for i, v in possible_anomalies[:20]:\n",
    "            red = '0'\n",
    "            for a,b in red_events:\n",
    "                if a == i:\n",
    "                    red = '1'\n",
    "                    break\n",
    "            line = '{0},{1},{2}\\n'.format(d, v, red)\n",
    "            outfile.write(line)\n",
    "      \n",
    "    # Save model to a file\n",
    "    model_filepath = '{0}/{1}_simple_lm.hdfs'.format(users_modeldir, user_name)\n",
    "\n",
    "    tf.keras.models.save_model(\n",
    "        model,\n",
    "        model_filepath,\n",
    "        overwrite=True,\n",
    "        include_optimizer=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(users_lossdir):\n",
    "    os.makedirs(users_lossdir)\n",
    "\n",
    "if not os.path.exists(users_modeldir):\n",
    "    os.makedirs(users_modeldir)\n",
    "    \n",
    "for u in user_names:\n",
    "    print('Processing files for User:', u)\n",
    "    outfile_name = \"{0}/{1}_losses.txt\".format(users_lossdir, u)\n",
    "\n",
    "    with open(outfile_name, 'w') as outfile:\n",
    "        outfile.write('day,loss,redevent\\n')\n",
    "        process_user(u, outfile)\n",
    "        outfile.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

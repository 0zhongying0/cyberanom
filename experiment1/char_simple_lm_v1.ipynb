{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN char language model for LANL\n",
    "\n",
    "This version is based on simple_lm.py file without all the generic params. We are just attempting to replicate the work in this notebook.\n",
    "\n",
    "Simple RNN not bidirctional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "tf.set_random_seed(613)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "np.random.seed(613)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fan_scale(initrange, activation, tensor_in):\n",
    "    \"\"\"\n",
    "    Creates a scaling factor for weight initialization according to best practices.\n",
    "\n",
    "    :param initrange: Scaling in addition to fan_in scale.\n",
    "    :param activation: A tensorflow non-linear activation function\n",
    "    :param tensor_in: Input tensor to layer of network to scale weights for.\n",
    "    :return: (float) scaling factor for weight initialization.\n",
    "    \"\"\"\n",
    "    if activation == tf.nn.relu:\n",
    "        initrange *= np.sqrt(2.0/float(tensor_in.get_shape().as_list()[1]))\n",
    "    else:\n",
    "        initrange *= (1.0/np.sqrt(float(tensor_in.get_shape().as_list()[1])))\n",
    "    return initrange\n",
    "\n",
    "\n",
    "def batch_softmax_dist_loss(truth, h, dimension, scale_range=1.0):\n",
    "    \"\"\"\n",
    "    This function paired with a tensorflow optimizer is multinomial logistic regression.\n",
    "    It is designed for cotegorical predictions.\n",
    "\n",
    "    :param truth: (tf.Tensor) A tensorflow vector tensor of integer class labels.\n",
    "    :param h: (tf.Tensor) A placeholder if doing simple multinomial logistic regression, or the output of some neural network.\n",
    "    :param dimension: (int) Number of classes in output distribution.\n",
    "    :param scale_range: (float) For scaling the weight matrices (by default weights are initialized two 1/sqrt(fan_in)) for tanh activation and sqrt(2/fan_in) for relu activation.\n",
    "    :return: (tf.Tensor, shape = [MB, Sequence_length]) Cross-entropy of true distribution vs. predicted distribution.\n",
    "    \"\"\"\n",
    "    fan_in = h[0].get_shape().as_list()[1]\n",
    "    initializer = fan_scale(scale_range, tf.tanh, h[0]) * tf.truncated_normal([fan_in, dimension],\n",
    "                                                                               dtype=tf.float32,\n",
    "                                                                               name='W')\n",
    "    U = tf.get_variable('softmax_weights', initializer=initializer)\n",
    "\n",
    "    hidden_tensor = tf.stack(h) # sequence_length X batch_size X final_hidden_size\n",
    "    tf.add_to_collection('logit_weights', U)\n",
    "    b = tf.get_variable('softmax_bias', initializer=tf.zeros([dimension]))\n",
    "    ustack = tf.stack([U]*len(h)) #sequence_length X final_hidden_size X dimension\n",
    "    logits = tf.matmul(hidden_tensor, ustack) + b # sequence_length X batch_size X dimension\n",
    "    logits = tf.transpose(logits, perm=[1, 0, 2]) # batch_size X sequence_length X dimension\n",
    "    tf.add_to_collection(\"true_probabilities\", tf.nn.softmax(logits)) # added to store probabilities of true logline\n",
    "    loss_matrix = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=truth) # batch_size X sequence_length\n",
    "    return loss_matrix\n",
    "\n",
    "\n",
    "def get_mask(lens, num_tokens):\n",
    "    \"\"\"\n",
    "    For masking output of lm_rnn for jagged sequences for correct gradient update.\n",
    "    Sequence length of 0 will output nan for that row of mask so don't do this.\n",
    "\n",
    "    :param lens: Numpy vector of sequence lengths\n",
    "    :param num_tokens: (int) Number of predicted tokens in sentence.\n",
    "    :return: A numpy array mask MB X num_tokens\n",
    "             For each row there are: lens[i] values of 1/lens[i]\n",
    "                                     followed by num_tokens - lens[i] zeros\n",
    "    \"\"\"\n",
    "    mask_template = np.repeat(np.arange(num_tokens).reshape(1, -1), lens.shape[0], axis=0)\n",
    "    return (mask_template < lens.reshape([-1, 1])).astype(float) / lens.reshape([-1, 1]).astype(float)\n",
    "\n",
    "\n",
    "def write_results(datadict, pointloss, outfile, batch):\n",
    "    \"\"\"\n",
    "    Writes loss for each datapoint, along with meta-data to file.\n",
    "\n",
    "    :param datadict: Dictionary of data names (str) keys to numpy matrix values for this mini-batch.\n",
    "    :param pointloss: MB X 1 numpy array\n",
    "    :param outfile: Where to write results.\n",
    "    :param batch: The mini-batch number for these events.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for line, sec, day, usr, red, loss in zip(datadict['line'].flatten().tolist(),\n",
    "                                              datadict['second'].flatten().tolist(),\n",
    "                                              datadict['day'].flatten().tolist(),\n",
    "                                              datadict['user'].flatten().tolist(),\n",
    "                                              datadict['red'].flatten().tolist(),\n",
    "                                              pointloss.flatten().tolist()):\n",
    "        outfile.write('%s %s %s %s %s %s %r\\n' % (batch, line, sec, day, usr, red, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_rnn(x, t, token_embed, layers, seq_len=None, context_vector=None, cell=tf.contrib.rnn.BasicLSTMCell):\n",
    "    \"\"\"\n",
    "    Token level LSTM language model that uses a sentence level context vector.\n",
    "\n",
    "    :param x: (tensor) Input to rnn\n",
    "    :param t: (tensor) Targets for language model predictions (typically next token in sequence)\n",
    "    :param token_embed: (tensor) MB X ALPHABET_SIZE.\n",
    "    :param layers: A list of hidden layer sizes for stacked lstm\n",
    "    :param seq_len: A 1D tensor of mini-batch size for variable length sequences\n",
    "    :param context_vector: (tensor) MB X 2*CONTEXT_LSTM_OUTPUT_DIM. Optional context to append to each token embedding\n",
    "    :param cell: (class) A tensorflow RNNCell sub-class\n",
    "    :return: (tuple) token_losses (tensor), hidden_states (list of tensors), final_hidden (tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    token_set_size = token_embed.get_shape().as_list()[0]\n",
    "    cells = [cell(num_units) for num_units in layers]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    # mb X sentence_length X embedding_size\n",
    "    x_lookup = tf.nn.embedding_lookup(token_embed, x)\n",
    "\n",
    "    # List of mb X embedding_size tensors\n",
    "    input_features = tf.unstack(x_lookup, axis=1)\n",
    "\n",
    "    # input_features: list max_length of sentence long tensors (mb X embedding_size+context_size)\n",
    "    if context_vector is not None:\n",
    "        input_features = [tf.concat([embedding, context_vector], 1) for embedding in input_features]\n",
    "\n",
    "    scope = tf.VariableScope(name= 'language_model', reuse = None)\n",
    "    \n",
    "    # hidden_states: sentence length long list of tensors (mb X final_layer_size)\n",
    "    # cell_state: data structure that contains the cell state for each hidden layer for a mini-batch (complicated)\n",
    "    # Creates a recurrent neural network specified by RNNCell cell\n",
    "    hidden_states, cell_state = tf.contrib.rnn.static_rnn(cell, input_features,\n",
    "                                          initial_state=None,\n",
    "                                          dtype=tf.float32,\n",
    "                                          sequence_length=seq_len,\n",
    "                                          scope=scope)\n",
    "\n",
    "    # batch_size X sequence_length (see tf_ops for def)\n",
    "    token_losses = batch_softmax_dist_loss(t, hidden_states, token_set_size)\n",
    "    final_hidden = cell_state[-1].h\n",
    "    return token_losses, hidden_states, final_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OnlineBatcher:\n",
    "    \"\"\"\n",
    "    Gives batches from a csv file.\n",
    "    For batching data too large to fit into memory. Written for one pass on data!!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datafile, batch_size,\n",
    "                 skipheader=False, delimiter=',',\n",
    "                 alpha=0.5, size_check=None,\n",
    "                 datastart_index=3, norm=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param datafile: (str) File to read lines from.\n",
    "        :param batch_size: (int) Mini-batch size.\n",
    "        :param skipheader: (bool) Whether or not to skip first line of file.\n",
    "        :param delimiter: (str) Delimiter of csv file.\n",
    "        :param alpha: (float)  For exponential running mean and variance.\n",
    "                      Lower alpha discounts older observations faster.\n",
    "                      The higher the alpha, the further you take into consideration the past.\n",
    "        :param size_check: (int) Expected number of fields from csv file. Used to check for data corruption.\n",
    "        :param datastart_index: (int) The csv field where real valued features to be normalized begins.\n",
    "                                Assumed that all features beginnning at datastart_index till end of line\n",
    "                                are real valued.\n",
    "        :param norm: (bool) Whether or not to normalize the real valued data features.\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.f = open(datafile, 'r')\n",
    "        self.batch_size = batch_size\n",
    "        self.index = 0\n",
    "        self.delimiter = delimiter\n",
    "        self.size_check = size_check\n",
    "        if skipheader:\n",
    "            self.header = self.f.readline()\n",
    "        self.datastart_index = datastart_index\n",
    "        self.norm = norm\n",
    "        self.replay = False\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        :return: (np.array) until end of datafile, each time called,\n",
    "                 returns mini-batch number of lines from csv file\n",
    "                 as a numpy array. Returns shorter than mini-batch\n",
    "                 end of contents as a smaller than batch size array.\n",
    "                 Returns None when no more data is available(one pass batcher!!).\n",
    "        \"\"\"\n",
    "        matlist = []\n",
    "        l = self.f.readline()\n",
    "        if l == '':\n",
    "            return None\n",
    "        rowtext = np.array([float(k) for k in l.strip().split(self.delimiter)])\n",
    "        if self.size_check is not None:\n",
    "            while len(rowtext) != self.size_check:\n",
    "                l = self.f.readline()\n",
    "                if l == '':\n",
    "                    return None\n",
    "                rowtext = np.array([float(k) for k in l.strip().split(self.delimiter)])\n",
    "        matlist.append(rowtext)\n",
    "        for i in range(self.batch_size - 1):\n",
    "            l = self.f.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            rowtext = np.array([float(k) for k in l.strip().split(self.delimiter)])\n",
    "            if self.size_check is not None:\n",
    "                while len(rowtext) != self.size_check:\n",
    "                    l = self.f.readline()\n",
    "                    if l == '':\n",
    "                        return None\n",
    "                    rowtext = np.array([float(k) for k in l.strip().split(self.delimiter)])\n",
    "            matlist.append(rowtext)\n",
    "        data = np.array(matlist)\n",
    "        if self.norm:\n",
    "            batchmean, batchvariance = data[:,self.datastart_index:].mean(axis=0), data[:, self.datastart_index:].var(axis=0)\n",
    "            if self.index == 0:\n",
    "                self.mean, self.variance = batchmean, batchvariance\n",
    "            else:\n",
    "                self.mean = self.alpha * self.mean + (1 - self.alpha) * batchmean\n",
    "                self.variance = self.alpha * self.variance + (1 - self.alpha) * batchvariance\n",
    "                data[:, self.datastart_index:] = (data[:, self.datastart_index:] - self.mean)/(self.variance + 1e-10)\n",
    "        self.index += self.batch_size\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_training_utils import EarlyStop\n",
    "\n",
    "def trainday(model, is_training, datafile_name, outfile, mb, bidir, x_shape, jagged = True, skipsos = True, verbose = False):\n",
    "    \n",
    "    jag = int(jagged) # Whether using sequences of variable length (Input should be zero-padded to max_sequence_length.'\n",
    "    skipsos = int(skipsos) # 'Whether to skip a start of sentence token.\n",
    "\n",
    "    batch_num = 0\n",
    "    data = OnlineBatcher(datafile_name, mb, delimiter=' ')\n",
    "    raw_batch = data.next_batch()\n",
    "    print('raw_batch.shape:', raw_batch.shape)\n",
    "    current_loss = sys.float_info.max\n",
    "    not_early_stop = EarlyStop()\n",
    "    endx = raw_batch.shape[1] - int(not bidir)\n",
    "    endt = raw_batch.shape[1] - int(bidir)\n",
    "    continue_training = not_early_stop(raw_batch, current_loss)\n",
    "    while continue_training:  # mat is not None and self.badcount < self.badlimit and loss != inf, nan:\n",
    "        datadict = {'line': raw_batch[:, 0],\n",
    "                    'second': raw_batch[:, 1],\n",
    "                    'day': raw_batch[:, 2],\n",
    "                    'user': raw_batch[:, 3],\n",
    "                    'red': raw_batch[:, 4],\n",
    "                    'x': raw_batch[:, (5+jag+skipsos):endx],\n",
    "                    't': raw_batch[:, (6+jag+skipsos):endt]}\n",
    "        if jagged:\n",
    "            datadict['lengths'] = raw_batch[:, 5]\n",
    "            datadict['mask'] = get_mask(datadict['lengths']-2*bidir-skipsos, sentence_length-2*bidir)\n",
    "#             print('datadict_length:', datadict['lengths'])\n",
    "#             print('datadict_mask:', datadict['mask'])\n",
    "            assert np.all(datadict['lengths'] <= x_shape.as_list()[1]), 'Sequence found greater than num_tokens_predicted'\n",
    "            assert np.nonzero(datadict['lengths'])[0].shape[0] == datadict['lengths'].shape[0], \\\n",
    "                'Sequence lengths must be greater than zero.' \\\n",
    "                'Found zero length sequence in datadict[\"lengths\"]: %s' % datadict['lengths']\n",
    "        eval_tensors = [avgloss, line_losses]\n",
    "        _, current_loss, pointloss = model.train_step(datadict, eval_tensors,\n",
    "                                                          update=is_training)\n",
    "        if not is_training:\n",
    "            write_results(datadict, pointloss, outfile, batch_num)\n",
    "        batch_num += 1\n",
    "        if verbose or (batch_num %999)==0:\n",
    "            print('%s %s %s %s %s %s %r' % (raw_batch.shape[0],\n",
    "                                            datadict['line'][0],\n",
    "                                            datadict['second'][0],\n",
    "                                            ('fixed', 'update')[is_training],\n",
    "                                            datafile_name,\n",
    "                                            data.index,\n",
    "                                            current_loss))\n",
    "        raw_batch = data.next_batch()\n",
    "        continue_training = not_early_stop(raw_batch, current_loss)\n",
    "        if continue_training < 0:\n",
    "            exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/0.txt  - training == True\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 3848637.0 29786.0 update ./data/char_feats/0.txt 1022976 0.77021384\n",
      "1024 6535995.0 40117.0 update ./data/char_feats/0.txt 2045952 0.5266479\n",
      "1024 9140719.0 51089.0 update ./data/char_feats/0.txt 3068928 0.45972463\n",
      "1024 11939260.0 63246.0 update ./data/char_feats/0.txt 4091904 0.42087907\n",
      "1024 15724098.0 86279.0 update ./data/char_feats/0.txt 5114880 0.39120644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/0.txt  - training == False\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 19566204.0 110070.0 fixed ./data/char_feats/1.txt 1022976 0.40691057\n",
      "1024 22299357.0 121382.0 fixed ./data/char_feats/1.txt 2045952 0.42088857\n",
      "1024 24919790.0 131706.0 fixed ./data/char_feats/1.txt 3068928 0.42149878\n",
      "1024 27604482.0 142062.0 fixed ./data/char_feats/1.txt 4091904 0.41650963\n",
      "1024 30713235.0 156981.0 fixed ./data/char_feats/1.txt 5114880 0.39003956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/1.txt  - training == True\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 19566204.0 110070.0 update ./data/char_feats/1.txt 1022976 0.39661333\n",
      "1024 22299357.0 121382.0 update ./data/char_feats/1.txt 2045952 0.40255657\n",
      "1024 24919790.0 131706.0 update ./data/char_feats/1.txt 3068928 0.39973414\n",
      "1024 27604482.0 142062.0 update ./data/char_feats/1.txt 4091904 0.39186388\n",
      "1024 30713235.0 156981.0 update ./data/char_feats/1.txt 5114880 0.36862835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/1.txt  - training == False\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 37034163.0 196201.0 fixed ./data/char_feats/2.txt 1022976 0.38579068\n",
      "1024 39963088.0 210078.0 fixed ./data/char_feats/2.txt 2045952 0.39187747\n",
      "1024 42756960.0 223183.0 fixed ./data/char_feats/2.txt 3068928 0.39263672\n",
      "1024 45857463.0 239684.0 fixed ./data/char_feats/2.txt 4091904 0.38157198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/2.txt  - training == True\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 37034163.0 196201.0 update ./data/char_feats/2.txt 1022976 0.38498858\n",
      "1024 39963088.0 210078.0 update ./data/char_feats/2.txt 2045952 0.39046043\n",
      "1024 42756960.0 223183.0 update ./data/char_feats/2.txt 3068928 0.39088514\n",
      "1024 45857463.0 239684.0 update ./data/char_feats/2.txt 4091904 0.38063815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/2.txt  - training == False\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 52637064.0 283730.0 fixed ./data/char_feats/3.txt 1022976 0.37970373\n",
      "1024 56172011.0 306028.0 fixed ./data/char_feats/3.txt 2045952 0.37221843\n",
      "1024 59672552.0 328265.0 fixed ./data/char_feats/3.txt 3068928 0.37211424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/3.txt  - training == True\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 52637064.0 283730.0 update ./data/char_feats/3.txt 1022976 0.37957034\n",
      "1024 56172011.0 306028.0 update ./data/char_feats/3.txt 2045952 0.37181833\n",
      "1024 59672552.0 328265.0 update ./data/char_feats/3.txt 3068928 0.37170434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/3.txt  - training == False\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 65886461.0 368339.0 fixed ./data/char_feats/4.txt 1022976 0.37197787\n",
      "1024 69451727.0 390989.0 fixed ./data/char_feats/4.txt 2045952 0.37134576\n",
      "1024 73031187.0 413857.0 fixed ./data/char_feats/4.txt 3068928 0.37355188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/4.txt  - training == True\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 65886461.0 368339.0 update ./data/char_feats/4.txt 1022976 0.3719589\n",
      "1024 69451727.0 390989.0 update ./data/char_feats/4.txt 2045952 0.37132\n",
      "1024 73031187.0 413857.0 update ./data/char_feats/4.txt 3068928 0.37354654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/4.txt  - training == False\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 79479448.0 456052.0 fixed ./data/char_feats/5.txt 1022976 0.39141363\n",
      "1024 82126356.0 465753.0 fixed ./data/char_feats/5.txt 2045952 0.39211586\n",
      "1024 84547732.0 474578.0 fixed ./data/char_feats/5.txt 3068928 0.3869393\n",
      "1024 86974174.0 483519.0 fixed ./data/char_feats/5.txt 4091904 0.39347082\n",
      "1024 89438354.0 492534.0 fixed ./data/char_feats/5.txt 5114880 0.39215887\n",
      "1024 92659342.0 509168.0 fixed ./data/char_feats/5.txt 6137856 0.37553963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/5.txt  - training == True\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 79479448.0 456052.0 update ./data/char_feats/5.txt 1022976 0.39141214\n",
      "1024 82126356.0 465753.0 update ./data/char_feats/5.txt 2045952 0.39209676\n",
      "1024 84547732.0 474578.0 update ./data/char_feats/5.txt 3068928 0.3869223\n",
      "1024 86974174.0 483519.0 update ./data/char_feats/5.txt 4091904 0.39344972\n",
      "1024 89438354.0 492534.0 update ./data/char_feats/5.txt 5114880 0.39213938\n",
      "1024 92659342.0 509168.0 update ./data/char_feats/5.txt 6137856 0.3755257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ./data/char_feats/5.txt  - training == False\n",
      "raw_batch.shape: (1024, 121)\n",
      "1024 97720633.0 537099.0 fixed ./data/char_feats/6.txt 1022976 0.3711587\n",
      "elapsed time: 14213.488190889359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done Training. End of data stream."
     ]
    }
   ],
   "source": [
    "from graph_training_utils import ModelRunner\n",
    "\n",
    "sentence_length = 114 # based on the max_line_lenght when doing preprocessing of the data\n",
    "token_set_size  = 96  # taken from <safekit>/features/specs/lm/lanl_char_config.json\n",
    "learnrate = 0.001\n",
    "debug = False\n",
    "jagged = False\n",
    "bidir = False\n",
    "\n",
    "\n",
    "em_size = 20 # Size of embeddings for categorical features. This is the default\n",
    "token_embed = tf.Variable(tf.truncated_normal([token_set_size, em_size]))  # Initial embeddings vocab X embedding size\n",
    "\n",
    "lm_layers = [10] # A list of hidden layer sizes. this is the default\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, sentence_length])\n",
    "t = tf.placeholder(tf.int32, [None, sentence_length])\n",
    "\n",
    "# TBD... figure out what's the ident_ran_and ran are used for...\n",
    "#        cell_type = 'lstm' # Can be either \"lstm\", \"ident_ran\", or \"ran\"'\n",
    "\n",
    "ph_dict = {'x': x, 't': t}\n",
    "\n",
    "# in the original code seq_len is only set if the args.jagged is true\n",
    "# jagged is defined as \"Whether using sequences of variable length \n",
    "#              (Input should be zero-padded to max_sequence_length.\n",
    "# I believe originally they used jagged for the char language model.\n",
    "if jagged:\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    ph_dict['lengths'] = seq_len\n",
    "else:\n",
    "    seq_len = None\n",
    "    \n",
    "\n",
    "token_losses, hidden_states, final_hidden = lm_rnn(x, t, token_embed,\n",
    "                                                               lm_layers, seq_len=seq_len,\n",
    "                                                               cell=tf.contrib.rnn.BasicLSTMCell)\n",
    "\n",
    "# \n",
    "if jagged:\n",
    "    ph_dict['mask'] = tf.placeholder(tf.float32, [None, sentence_length])\n",
    "    token_losses *= ph_dict['mask']\n",
    "    line_losses = tf.reduce_sum(token_losses, axis=1)  # batch_size X 1\n",
    "else:\n",
    "    line_losses = tf.reduce_mean(token_losses, axis=1)  # batch_size X 1\n",
    "\n",
    "avgloss = tf.reduce_mean(line_losses)  # scalar\n",
    "\n",
    "model = ModelRunner(avgloss, ph_dict, learnrate=learnrate, debug=debug,\n",
    "                    decay=True,\n",
    "                    decay_rate=0.99, decay_steps=20)\n",
    "\n",
    "# training loop\n",
    "start_time = time.time()\n",
    "\n",
    "data_dir = './data/char_feats/'\n",
    "results_dir = './lanl_result/'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "outfile_name = 'simple_' + time.ctime(time.time()).replace(' ', '-')\n",
    "\n",
    "outfile = open(results_dir + outfile_name, 'w')\n",
    "\n",
    "num_days = 7 # that's what we have in our current testing env.\n",
    "mb_size = 1024 # 512\n",
    "\n",
    "files = [data_dir + str(i) + '.txt' for i in range(num_days)]\n",
    "#print('list of files:', files)\n",
    "outfile.write(\"batch line second day user red loss\\n\")\n",
    "\n",
    "for idx, fname in enumerate(files[:-1]):\n",
    "    print('processing file', fname, ' - training == True')\n",
    "    trainday(model, True, fname, outfile, mb_size, bidir, x.get_shape(), jagged = jagged)\n",
    "    print('processing file', fname, ' - training == False')\n",
    "    trainday(model, False, files[idx + 1], outfile, mb_size, bidir, x.get_shape(), jagged = jagged)\n",
    "    \n",
    "outfile.close()\n",
    "total_time = time.time() - start_time\n",
    "print('elapsed time: %s' % total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

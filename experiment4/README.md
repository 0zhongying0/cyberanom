## Experiment 4

### This implementation is similar to experiment4 processing but use a different model with attention added.

### The model used is the Transformer model in the paper ()[] but we didn't use the official Tensor2Tensor implementation
### from Tensorflow. Here we use the simple implemenation from (github)[https://github.com/Lsdefine/attention-is-all-you-need-keras]
